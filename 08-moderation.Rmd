# Moderation with Regression Analysis {#moderation}
> Key concepts: interaction variable, covariate, outcome, regression equation, dummy variables, normally distributed residuals, linearity, homoscedasticity, independent observations, statistical diagram, common support, simple slope, conditional effect, mean-centering.

```{r eval=FALSE, echo=FALSE}
* Add SPSS video explaining how to report regresision results? See Fam?
```

### Summary {-}

> My outcome is numeric but at least one predictor is also numeric, so I cannot apply analysis of variance. How can I investigate moderation with regression analysis?

The linear regression model is a powerful and very popular tool for predicting a numeric outcome variable from one or more predictor variables. In this chapter, we use regression to evaluate the effects of an anti-smoking campaign. We predict attitude towards smoking from exposure to the anti-smoking campaign (numeric), time spent with smokers (numeric), and the respondent's smoking status (categorical). 

Regression coefficients, that is, the slopes of regression lines, are the effects in a regression model. They show the predicted difference in the outcome for a one unit difference in the predictor (exposure, time spent with smokers) or the predicted mean difference for two categories (smokers versus non-smokers). 

But what if the predictive effect is not the same in all contexts? For example, exposure to an anti-smoking campaign may generally generate a more negative attitude towards smoking. The effect, however, is probably different for people who smoke than for people who do not smoke. In this case, the effect of campaign exposure on attitude towards smoking is moderated by context: Whether or not the person exposed to the campaign is a smoker.

Different effect sizes for different contexts are different regression coefficients for different contexts. We need different regression lines for different groups of people. We can use an interaction variable as a predictor in a regression model to accommodate for moderation as different slopes. An interaction variable is just the product of the predictor and moderator variables.

As a predictor in the model, an interaction variable has a confidence interval and a p value. The confidence interval tells us the plausible values for the size of the interaction effect in the population. The p value tests the null hypothesis that there is no interaction effect at all in the population.

To interpret the interaction effect, we must determine the size of the effect of the predictor on the outcome variable for several interesting values of the moderator. If the moderator is categorical, we want to know the effect (simple slope) within each category of the moderator. For example, the effect of campaign exposure on smoking attitude for smokers and the effect for non-smokers. 

The moderator can also be a continuous variable, for example, the time a person spends with people who smoke. In this case, we may look at the effect for the mean value of the moderator (moderate score level) and one standard deviation below (low level) or above (high level) the mean.

An interaction effect in a regression model closely resembles an interaction effect in analysis of variance. In contrast, the effect of a single predictor in a regression model is not a main effect as in analysis of variance. It is a conditional effect, namely the effect for one particular value of the moderator (context). To understand this, we must pay close attention to the regression equation.

```{r SPSS-PROCESS, eval=FALSE, echo=FALSE}
# TERMINOLOGY: predictor (X, cause), moderator (M, represents context), covariate (C, control), outcome (Y).

# SPSS versus PROCESS:
# + SPSS: visual checks on residuals: normal distribution and zpred by zresid (linearity, homoscedasticity)
# + SPSS: scatterplot (X, Y) with regression lines per group (Moderator) with original variable and value labels, showing common support ; add reference line for each group manually specifying the regression equation, setting covariates to their mean values (with categorical moderator and no covariates or covariates that are not correlated with the predictor, Regression Variable Plots can be used)
# - SPSS: manual entering of regression equation with selected values for covariates (and a continuous moderator; lines can only be labeled with the equation text)
# - SPSS: interaction predictors have to be created by hand (also multiple interaction variables for a categorical predictor; Transform>Create Dummy Variables, taught in RMCS?)
# - SPSS: mean-centering must be done by hand
# - SPSS: statistical inference for non-zero moderator values requires separate regression models where the low category requires ADDING one SD instead of subtracting.
# + PROCESS: must be used anyway for mediation models
# - PROCESS: no visual checks on assumptions
# - PROCESS: no visual impression of common support of predictor for different values of the moderator (requires additional work with continuous moderator also in SPSS)
# - PROCESS: data list for visualization of results must be copied from output to syntax file, variable and value labels must be added, lines must be added (and this requires that the moderator has no decimal places in SPSS?) in chart editor
# - PROCESS: model number must be remembered
# - PROCESS: because the student need not create the interaction variables, mean-center or "re-center" for probing the interaction, PROCESS output is more mysterious (but the estimated slopes for different moderator values are directly linked to the graph)
# - PROCESS: dichotomies are automatically treated as indicator variables but categorical predictors/moderators are treated as numeric ; it is not possible to use more than one moderator variable, so PROCESS cannot handle a categorical moderator.
# DECISION: Use PROCESS for results and interpretation. Check assumptions with SPSS (interaction variables and, possibly, dummies must be created but no need for mean-centering) or forget about assumptions.
```

## The Regression Equation {#regression-equation}

In the social sciences, we usually expect that a particular outcome has several causes. Investigating the effects of an anti-smoking campaign, for instance, we would not assume that a person's attitude towards smoking depends only on exposure to a particular anti-smoking campaign. It is easy to think of other and perhaps more influential causes such as personal smoking status, contact with people who do or do not smoke, susceptibility to addiction, and so on.

```{r concept-smoke, echo=FALSE, fig.asp=0.4, fig.cap="A conceptual model with some hypothesized causes of attitude towards smoking."}
# Draw conceptual diagram: Attitude towards smoking predicted by Exposure, Smoking status, and Contact with smokers.
library(ggplot2)
# Create coordinates for the variable names.
variables <- data.frame(x = c(0.3, 0.3, 0.3, 0.7), 
                        y = c(.1, .3, .5, .3),
                        label = c("Exposure", "Smoking Status", "Contact with Smokers", "Attitude"))
ggplot(variables, aes(x, y)) + 
  geom_segment(aes(x = variables$x[1], y = variables$y[1], xend = variables$x[4] - 0.04, yend = variables$y[4] - 0.02), arrow = arrow(length = unit(0.04, "npc"), type = "closed")) + 
  geom_segment(aes(x = variables$x[2], y = variables$y[2], xend = variables$x[4] - 0.04, yend = variables$y[4]), arrow = arrow(length = unit(0.04, "npc"), type = "closed")) + 
  geom_segment(aes(x = variables$x[3], y = variables$y[3], xend = variables$x[4] - 0.04, yend = variables$y[4] + 0.02), arrow = arrow(length = unit(0.04, "npc"), type = "closed")) + 
  geom_label(aes(label=label)) + 
  coord_cartesian(xlim = c(0.2, 0.8), ylim = c(0.05, 0.55)) +
  theme_void()
# Cleanup.
rm(variables)
```

Figure \@ref(fig:concept-smoke) summarizes some hypothesized causes of the attitude towards  smoking. A regression model translates this conceptual diagram into a statistical model. The statistical regression model is a mathematical function with the outcome variable (also known as the dependent variable, usually referred to with the letter $y$) as the sum of a constant ($a$), the effects ($b$) of predictors ($x$), which are _predictive effects_, and an error term ($e$), which is also called the _residuals_, see Equation \@ref(eq:regression).

\begin{equation} 
\small
  y = a + b_1*x_1 + b_2*x_2 + b_3*x_3 + e 
  (\#eq:regression) 
\normalsize
\end{equation} 

### Interpretation of a regression equation

Let us first have a close look at a simple regression equation, that is, a regression equation with just one predictor ($x$).

```{r multiple-regression, fig.cap="What are the function and meaning of the constant and regression weights?", eval=FALSE, echo=FALSE}
#DROPPED.

# Goal: Refresh function and interpretation of constant and regression weights in a multiple regression model.
# Generate data (N = 40) for a regression model predicting attitude with the parameters as displayed in the inputs, e.g., constant = 0.4, b1 (exposure) = -0.6, b2 (smoking status) = 1.6, b3 (smoker contact) = 0.2 and an error term rnorm(0, 1). Range for attitude is [-5, +5], range for exposure and contact is [0, 10], values for smokings status is 0 (non-smoker) and 1 (smoker). Draw a scatterplot for each attitude (Y) by each of the predictors with the regression line for the predictor (use the parameter setting for the constant and its slope and set the other predictors to zero). Allow the user to change each parameter within a reasonable range (slider?) and update all three scatterplots (don't update the simulated data). It would be nice if the original regression lines remain visible (gray) for comparison.

1. What does the constant mean and how do the regression lines change if you change the value of the constant? Use the regression equation to explain your answer.

2. What does a regression weight ($b$) mean and how does the regression line change if you change its value? Again, explain your answer.
```

```{r regression-coefficients, eval=TRUE, echo=FALSE, fig.cap="What is the meaning of the regression equation?"}
# REPLACED BY STATIC IMAGE

# Goal: Refresh interpretation of the unstandardized and standardized regression coefficient by manipulating the standard deviation of the outcome variable (minimum value is sd(x) * |b|).
# Generate data (N = 40) from a regression model (see below). Plot attitude (y) against (x), add regression line (fixed slope), and vertical lines from x = 5 and x = 6 to regression line and horizontal lines from where they meet the regression lines to the Y axis. The sd of the error term can be scaled (s) in the range [.5, 2] (larger error produces larger SD of y). Calculate the standardized coefficient (beta) and display as label in plot. Display the regression equation (y = 2.4 + -0.6*x + e). If the scale of the error term is changed, update the dots in the scatterplot and beta.
# # Generate predictor.
# x <- seq(from = 0.5, to = 9.5, length.out = 20)
# # Generate random errors.
# set.seed(1272)
# e <- rnorm(20, mean = 0, sd = 0.3)
# e <- sign(e) * sqrt(abs(e*abs(e) - mean(e*abs(e)))) # center in squares
# # Set scale of error term.
# s <- 1
# # Generate outcome scaled by unstandardized regression coefficient.
# y <- mean(x)/2 + -0.6*x + s*e
# # Calculate (approximate) standardized regression coefficient.
# beta = round(-0.6 + sd(x) / sd(y), digits = 2)

  source("../apps/plottheme/styling.R", local = TRUE)
  #CREATE PREDICTOR
  n <- 100 #number of data points
  const = 1.4 #regression constant
  expo = -0.25 #regression coefficient
  set.seed(4932)
  exposure <- runif(n) * 10
  # Create outcome.
  set.seed(390)
  attitude <- expo * exposure + rnorm(n, mean = const, sd = 1.4)
  # Collect in data frame.
  scatter <- data.frame(attitude = attitude, exposure = exposure)
  #PLOT
  ggplot(scatter[scatter$attitude > -5,]) +
      geom_point(shape = 21,
                 size = 3,
                 aes(x = exposure,
                     y = attitude)) +
      geom_segment(aes(x = 0, xend = 10, y = const, yend = (const + (10 * expo))),
                   size = 1.6, colour = brewercolors["Blue"]) +
      geom_segment(aes(x = 5, xend = 5, y = -5, yend = (const + 5*expo)),
                   size = 1) +
      geom_segment(aes(x = 0, xend = 5, y = (const + 5*expo), yend = (const + 5*expo)),
                   size = 1) +
      geom_segment(aes(x = 4, xend = 4, y = -5, yend = (const + 4*expo)),
                   size = 1) +
      geom_segment(aes(x = 0, xend = 4, y = (const + 4*expo), yend = (const + 4*expo)),
                   size = 1) +
      scale_x_continuous(limits = c(0, 10), breaks = seq(0, 10, 1), expand = c(0,0)) +
      scale_y_continuous(limits = c(-5, 5), breaks = c(-5, (const + 5*expo), (const + 4*expo), const, 5), expand = c(0,0)) +
      geom_text(aes(x = 6, y = 4), label = "y = a + b * x + e") +
      geom_text(aes(x = 6, y = 3.5), label = paste0("attitude = ", const, " + ", expo, " * exposure + e")) +
      labs("Exposure", "Attitude") +
      theme_general()
  rm(n, attitude, exposure, scatter, const, expo)
```

1. How does the plot visualize the constant of the regression equation?

```{r eval=FALSE}
* The constant of a regression equation, represented by the symbol a, is the
predicted value of the outcome variable if all predictors are zero.
* Graphically, this is where the regression line cuts the vertical (y) axis.
* In the plot, a is 1.4 according to the equations and the regression line cuts
the vertical axis at 1.4 as it should.
```

2. Explain how the horizontal and vertical lines in the plot help to interpret the unstandardized regression coefficient $b$.

```{r eval=FALSE}
* An unstandardized regression coefficient, denoted by the symbol b, tells us
the predicted difference in the outcome for a difference of one unit in the
predictor variable.
* According to the equations, the predicted attitude decreases by 0.25 for a one
unit difference (4 to 5) in exposure.
* This is the decrease from 0.40 to 0.15 signalled by the horizontal line
segments.
```

Good understanding of the regression equation is necessary for understanding moderation in regression models. So let us have a close look at an example equation (Eq. \@ref(eq:regrexample)). The outcome variable attitude towards smoking is predicted from a constant and three predictor variables.

\begin{equation}
\small
  attitude = constant + b_1*exposure + b_2*status + b_3*contact + e 
  (\#eq:regrexample) 
\normalsize
\end{equation}

The constant adds a fixed quantity to the predicted attitude for all participants. It adjusts the overall level of the predicted attitude. 

More precisely, the constant is the predicted attitude if a person scores zero on all predictor variables. To see this, plug in zero for all predictors in the equation (Eq. \@ref(eq:regsmokedummy)) and remember that zero times something is zero. This reduces the equation to the constant and the _error term_ $e$. The error term is the error of our prediction, also known as the _residual_. It does not help to predict the outcome, so the constant is the only remaining predictor.

\begin{equation}
\small
\begin{split}
  attitude &= constant + b_1*0 + b_2*0 + b_3*0 + e \\ 
  attitude &= constant + 0 + 0 + 0 + e \\
  attitude &= constant + e
\end{split}
  (\#eq:regsmokedummy) 
\normalsize
\end{equation}

For all persons scoring zero on exposure, smoking status, and contact with smokers, the predicted attitude equals the value of the regression constant. This interpretation only makes sense if the predictors can be zero. If they include, for instance, scales ranging from one to seven, there are no persons with zero scores on all predictors and the constant has no meaning.

The regression coefficients $b$ represent the predicted difference in the outcome for a difference of one unit in the predictor. For example, plug in the values 5 and 4 for the _status_ predictor in the equation. If we take the difference of the two equations, we are left with $b_1$. All other terms in the two equations cancel out (except, perhaps, the error term $e$).

\begin{equation}
\small
\begin{split}
  attitude = constant + b_1*5 + b_2*status + b_3*contact + e \\ 
  \underline{- \mspace{20mu} attitude = constant + b_1*4 + b_2*status + b_3*contact + e} \\
  attitude \mspace{4mu} difference = b_1*5 - b_1*4 = b_1*(5-4) =b_1
\end{split}
(\#eq:regweight) 
\normalsize
\end{equation}

We will be plugging in values for predictors in the regression equation a lot in this chapter. It is necessary for understanding and interpreting moderation.

### Continuous predictors

In a linear regression, the outcome variable ($y$) must be numeric and in principle continuous. There are regression models for other types of outcomes, for instance, logistic regression for a dichotomous (0/1) outcome and Poisson regression for a count outcome, but we will not discuss them.

The predictor variables must be either numeric or dichotomous. If exposure is measured as a scale, for instance ranging from zero to ten, the interpretation of the effect of exposure ($b_1$) is the one that we have encountered in the preceding section: the predicted difference in attitude (outcome variable) for a one unit difference in exposure (predictor variable) while all other predictor values do not change (are held constant).

Whether this predicted difference is small or large depends on the practical context: Is a small decrease in attitude towards smoking worth the effort of the campaign? If we want to apply a rule of thumb for the strength of the effect, we usually look at the standardized regression coefficient ($b^*$ according to APA6, _Beta_ in SPSS output). See Section \@ref(assoc-size) for some rules of thumb for effect size interpretation.

Note that the regression coefficient is calculated for the predictor values that occur within the data set. For example, if sample exposure scores are within the range three to seven, these values are used to predict attitude towards smoking. 

We cannot see this in the regression equation, which allows us to plug in -10, 0, or 100 as exposure values. But the values for attitude that we predict from these exposure values are probably nonsensical because our data do not tell us anything about the relation between exposure and anti-smoking attitude for predictor values outside the three to seven range. We should not pretend to know the effects of exposure levels outside this range. It is good practice to check the actual range of predictor values.

### Dichotomous predictors {#dichpredictor}

Instead of a numeric predictor, we can use a dichotomy as a predictor in a regression model. The dichotomy is preferably coded as 1 versus 0, for example, 1 for smokers and 0 for non-smokers among our respondents.

```{r regression-dichotomy, eval=TRUE, echo=FALSE, fig.cap="What is the difference in attitude between non-smokers and smokers?", echo=FALSE, screenshot.opts = list(delay = 5), dev="png", out.width="440px"}
# Goal: Refresh interpretation of unstandardized regression weight for a
# dichotomous predictor by manipulating group averages.
# Generate attitude scores  with average values -0.6 for non-smokers and 1.0 for
# smokers (N = 20 per group).
# Draw horizontal and vertical lines from the axes to the group means and add a
# regression line (line through the two group means).
# Display the current regression equation beneath or in the plot. 
# Allow the user to change the average score per group. Update the scatterplot,
# regression line and equation, and the horizontal/vertical lines.
knitr::include_app("http://82.196.4.233:3838/apps/regression-dichotomy/", height="630px")
```

1. What is the relation between the constant of the regression line in Figure \@ref(fig:regression-dichotomy) and group averages? Motivate your answer by changing the average attitude towards smoking for non-smokers in Figure \@ref(fig:regression-dichotomy). 

```{r eval=FALSE}
* The constant is equal to the average attitude of the non-smokers.
* Use the slider to change the average attitude towards smoking for
non-smokers.
* This will change the value of the constant in the regression equation. Why?
* Non-smokers score 0 on the (smoking) status variable. The regression
equation for non-smokers, then, is:
* attitude = constant + b * status = constant + b * 0 = constant
* Thus, we see that the predicted attitude for non-smokers equals the
constant. In addition, we know that the predicted value for a group
equals the average score of the group. As a result, the constant equals
the average score of non-smokers in this example.
* Note that this is true only if the group is coded zero and if the regression
model contains only one predictor (simple regression model).
```

2. Change the group means to detect the relation between group means and the unstandardized regression coefficient ($b$). How can we calculate the unstandardized regression coefficient ($b$) from the group averages? Show that your answer is correct by changing average attitude towards smoking with the slider for smokers. 

```{r eval=FALSE}
* In this example, the unstandardized regression coefficient (b) is equal to
the average attitude score of smokers minus the average attitude score of
non-smokers.
* This is so because smokers are coded as ones and non-smokers are coded as
zeros. The difference between smokers and non-smokers on the smoking status
variable is one. The regression coefficient (b) tells us the difference in
predicted attitude scores for a difference of one unit on the predictor
variable. As a result, the unstandardized regression coefficient (b) tells us
the difference between the average (= predicted value) for smokers and the
average (= predicted value) for non-smokers.
* With equations:
* Smokers average: attitude = constant + b * status = constant + b * 1 =
  = constant + b
* Non-smokers average: attitude = constant + b * status = constant + b * 0 =
  = constant
* Smokers - Non-smokers = (constant + b) - constant = b
```

Apart from numeric predictors, we can use dichotomous predictors, that is, predictors with only two values, which are preferably coded as 0 and 1 (_dummy coding_). The interpretation of the effect of a dichotomous predictor in a regression model is quite different from the interpretation of a numeric predictor.

For example, let us assume that smoking status is coded as smoker (1) versus non-smoker (0). Because this predictor can only take two values, we effectively have two versions of the regression equation. The first equation \@ref(eq:regdicho1) represents all smokers, so their smoking status score is 1. The smoking status of this group has a fixed contribution to the predicted average attitude, namely $b_2$.

\begin{equation}
\small
\begin{split}
  attitude &= constant + b_1*exposure + b_2*status + b_3*contact + e \\
  attitude &= constant + b_1*exposure + b_2*1 + b_3*contact + e \\
  attitude &= constant + b_1*exposure + b_2 + b_3*contact + e 
\end{split}
(\#eq:regdicho1) 
\normalsize
\end{equation}

Regression equation \@ref(eq:regdicho0) represents all non-smokers. Their smoking status score is 0, so the smoking status effect drops from the model. 

\begin{equation}
\small
\begin{split}
  attitude &= constant + b_1*exposure + b_2*status + b_3*contact + e \\
  attitude &= constant + b_1*exposure + b_2*0 + b_3*contact + e \\
  attitude &= constant + b_1*exposure + b_3*contact + e 
\end{split}
  (\#eq:regdicho0) 
\normalsize
\end{equation}

It does not make sense to interpret the regression coefficient of smoking status ($b_2$) as predicted difference in attitude for a difference of one 'more' smoking status. After all, the 0 and 1 scores do not mean that there is one unit 'more' smoking. Instead, the coefficient indicates that we are dealing with different groups: smokers versus non-smokers. We have to interpret the effect as a difference between two groups. More specifically, as the mean difference between the attitude of the group represented by the score 1 and the attitude of the _reference group_ represented by score 0.

If you compare the final equations for smokers (Eq. \@ref(eq:regdicho1)) and non-smokers (Eq. \@ref(eq:regdicho0)), the only difference is $b_2$, which is present for smokers but absent for non-smokers. It is the difference between the average outcome score (attitude) for smokers and the average outcome score (attitude) non-smokers. This is exactly the same as in an independent-samples t test!

Imagine that $b_2$ equals 1.6. This indicates that the average attitude towards smoking among smokers is 1.6 units above the average attitude among non-smokers. Is this a small or large effect? In the case of a dichotomous predictor, we should __not__ use the standardized regression coefficient to evaluate effect size. The standardized coefficient depends on the distribution of 1s and 0s, that is, which part of the respondents are smokers. But this should be irrelevant to the size of the effect. 

Therefore, it is recommended to interpret only the unstandardized regression coefficient for a dichotomous predictor. Interpret it as the difference in average outcome scores for two groups as we have done in the preceding paragraph.

### A categorical predictor and dummy variables {#categorical-predictor}

How about a categorical variable, that is, a variable containing three or more groups, for example, the distinction between respondents who smoke, former smokers, and respondents who never smoked? Can we include a categorical variable as a predictor in a regression model? Yes, we can but we need a trick.

```{r regression-categorical, fig.cap="What are the predictive effects of smoking status?", echo = FALSE, screenshot.opts = list(delay = 5), dev="png", out.width="420px"}
# Goal: Understanding effects of dummy variables by manipulating the reference
# group.
# Generate numerical outcome data (attitude, range [-5, 5]) for a categorical
# predictor with 3 categories: (1) non-smoker, (2) former smoker, (3) smoker,
# and a random choice of one of the following scenarios:
#* (1 M = -1.7, SD = 1) < (2) = (3 M = 0.75, SD = 1) {initial situation},
#* (1 M = -1.7, SD = 1) = (2) < (3 M = 0.75, SD = 1), 
#* (2  M = -1.7, SD = 1) < (1) = (3 M = 0.75, SD = 1),
#* (1 M = -1.7, SD = 1) < (2 M = 0.75, SD = 1) < (3 M = 1.8, SD = 1). 
# Display jittered scatterplot containing the three groups with group means
# indicated by a line segment and value, regression lines through the reference
# group mean and each of the other group means. Display b and p value of
# regression weights with the regression lines.
# Add input to select the reference group, initially set to group (1). Update
# regression lines and their p values on selection of a new reference group.
# Add button to generate a new plot (with a new scenario).
knitr::include_app("http://82.196.4.233:3838/apps/regression-categorical/", height="480px")
```

1. Interpret the effects of smoking status in Figure \@ref(fig:regression-categorical).

```{r eval=FALSE}
* The precise answer to this question obviously depends on the group means in
your plot.
* In general, the interpretation focuses on differences between the mean score
of the reference group and the mean scores of the other groups. In this
example, we are talking about average attitude towards smoking for each group
defined by their smoking status. The reference group is selected with the
Select reference drop-down list.
* The unstandardized regression coefficient (b) tells us how much larger
(positive coefficient) or smaller (negative coefficient) the mean score of a
group is in comparison to the reference group.
* The associated p value tells us how uncertain we are that there truly is a
mean difference in the population.
```

2. In the initial state of Figure \@ref(fig:regression-categorical), can you tell whether the attitude of smokers is significantly different from the attitude of former smokers? Select a different reference group to motivate your answer.

```{r eval=FALSE}
* In the initial state of this figure, people who never smoked are the
reference group. The p values, then, are associated with differences between on
the one hand people who never smoked and on the other hand people who stopped
smoking or are smoking. The comparison between the latter two is not included.
* It is hazardous to derive the p value of the difference between two
non-reference groups from the p values of the differences between these groups
and the reference group. Remember that a p value depends on both effect size
(difference between group means) and on the standard error. The latter depends
on the variance of the outcome variable per group (in the population) and on
sample size per group.
* The solution is to re-estimate the regression model with one of the groups
that you want to compare as reference group. In this figure, you can do this by
selecting a different group in the drop-down list.
```

3. Select some new plots. For each plot, determine which reference group you think is most convenient for summarizing the results. 

```{r eval=FALSE}
* There is not one right way of choosing a reference group; think about
arguments.
* 1. Substantive interest: Does your research focus on one particular group? If
so, use this group as the reference group, so it is included in all
comparisons. If, for example, the research is meant to support an anti-smoking
campaign, the group of current smokers is probably of central interest. Make
them your reference group.
* 2. If you expect a particular order in the group means, the group that you
expect to be in the middle is a good choice as reference group. If, for
example, you expect that attitude towards smoking is more positive for smokers
than for former smokers, and the latter are more positive than people who never
smoked, the former smokers are expected to be in the middle. If we use them as
reference group, we can test if they are more positive than people who never
smoked, and more negative than people who are currently smoking.
* 3. If two groups have relatively similar means in comparison to the third
group, you may be interested to know if the relatively small difference is
statistically significant. In this case, one of the two groups that have
similar means is a good reference group.
```

What if smoking status was measured with three categories: (1) never smoked, (2) have smoked, (3) currently smoking? We can only include such a categorical predictor if we change it into a set of dichotomies. 

A _categorical variable_ contains three or more categories or groups. We can create a new dichotomous variable for each group, indicating whether (score 1) or not (score 0) the respondent is part of this group. In the example, we could create the variables _neversmoked_, _smokesnomore_, and _smoking_. Every respondent would score 1 on one of the three variables and 0 on the other two variables. These variables are called _dummy variables_ or _indicator variables_.

```{r dummytable, echo=FALSE, screenshot.opts=list(delay = 2)}
knitr::kable(rbind(c("1 - Never smoked", "1", "0", "0"), 
                   c("2 - Former smoker", "0", "1", "0"),
                   c("3 - Smoker", "0", "0", "1")), 
             col.names = c("Original categorical variable:", "neversmoked", "smokesnomore", "smoking"), caption = "Dummy variables for a categorical predictor: One dummy variable is superfluous.", align = c("l", "c", "c", "c"))
```

If we want to include a categorical predictor in a regression model, we must use all dummy variables as predictors __except one__. In the example, we must include two out of the three dummy variables. We cannot include all three dummy variables because the score on the third dummy variable is determined by the score on the first two dummy variables. 

If a respondent scores 1 on either the Never Smoked or the Former Smoker dummy variables, she cannot be a smoker so her score must be 0 on the Smoker dummy variable. Reversely, someone who is not a member of the Never Smoked or Smokes No More Groups, must be a member of the Smoking Group. A person scoring 0 on the first two dummy variables, then, must score 1 on the third.

We cannot include a dummy variable as a predictor that is perfectly predictable from other dummy variables in the regression model. It is like including the same predictor twice: How can the estimation process decide which predictor is responsible for the effect? It can't decide, so the estimation process fails and no regression coefficients are estimated. If this happens, the predictors are said to be perfectly _multicollinear_.

The category or group that is left out of the regression model is the _reference group_. Remember that non-smokers were the reference group in the preceding section because the regression equation did not include a dichotomous predictor on which the non-smokers scored 1. The two groups were represented by only one dichotomy. As you see, we used one dummy less than the number of groups (two).

The interpretation of the effects (regression coefficients) for the included dummies is the same as for a single dichotomous predictor such as smoker versus non-smoker. It is the difference between average outcome score of the group scoring 1 on the dummy variable and the average outcome score of the reference group.

If we exclude the dummy variable for the respondents who never smoked, the regression weight of the dummy variable for the Former Smoker Group gives the average difference between former smokers and non-smokers. If the regression weight is negative, for instance -0.8, former smokers have a more negative attitude towards smoking than non-smokers. If the difference is positive, former smokers have a more positive attitude towards smoking. 

Which group should we use as reference category, that is, which dummy should not be used in the regression model? This is hard to say in general. If one group is of greatest interest to us, we could use this as the reference group, so all dummy variable effects express differences with this group. Alternatively, if we expect a particular ranking in average outcome scores, we may pick the group at the highest, lowest or middle rank as the reference group. If you can't decide, run the regression model several times with a different reference group.

### Sampling distributions and assumptions {#regr-inference}

```{r regression-sampling, eval=FALSE, echo=FALSE, fig.cap="What happens to regression lines from sample to sample?"}
# Goal: Understand that regression constant and coefficient(s) have sampling
# distributions.
# Generate a population with a weak negative effect (-0.6) of exposure on
# attitude and exposure, with a sizable error term (so a lot of variation in
# sample regression lines).
# Generate a sample (N = 10) and display it in a scatterplot with regression
# line, labelled with it's unstandardized regression coefficient value. Also
# plot the sampling distribution for the regression coefficient.
# Add a button to allow drawing a new sample; display the new sample and new
# regression line but retain the existing regression lines.
# Add button (or change sampling button) to draw 1,000 samples: don't display
# samples, just update sampling distribution with normal (or t) distribution as
# superimposed curve.

1. Which estimates can change from sample to sample: the regression constant, the regression coefficient, or both? Check your answer by drawing new samples.

2. What is the shape of the sampling distribution if you draw a lot of samples?

3. What happens if you draw samples of larger size? Think of what you learned in preceding chapters. Formulate your answer before you change sample size in Figure \@ref(fig:regression-sampling).
```

If we are working with a random sample or we have other reasons to believe that our data could have been different due to chance (Section \@ref(no-random-sample)), we should not just interpret the outcomes for the data set that we collected. We should apply statistical inference---confidence intervals and significance tests---to our results. The confidence interval gives us bounds for the population value of the unstandardized regression coefficient. The p value is used to test the _null hypothesis that the unstandardized regression coefficient is zero in the population_.

Each regression coefficient as well as the constant may vary from  sample to sample drawn from the same population, so we should devise a sampling distribution for each of them. These sampling distributions happen to have a t distribution under particular assumptions.

Chapters \@ref(param-estim) and \@ref(hypothesis) have extensively discussed how confidence intervals and p values are constructed and how they must be interpreted. So we may as well focus now on the assumptions under which the t distribution is a good approximation of the sampling distribution of a regression coefficient. 

#### Independent observations
The two most important assumptions require that the observations are _independent and identically distributed_. These requirements arise from probability theory. If they are violated, the statistical results should not be trusted.

Each observation, for instance, a measurement on a respondent, must be independent of all other observations. This respondent's outcome variable score is not allowed to depend on outcome scores of other respondents.

It is hardly possible to check that our observations are independent. We usually have to assume that this is the case. But there are situations in which we should not make this assumption. In time series data, for example, the daily amount of political news, we usually have trends, cyclic movements, or issues that affect the amount of news over a period of time. As a consequence, the amount and contents of political news on one day may depend on the amount and contents of political news on the preceding days.

Clustered data should also not be considered as independent observations. Think, for instance, of student evaluations of statistics tutorials. Students in the same tutorial group are likely to give similar evaluations because they had the same tutor and because of group processes: Both enthusiasm and dissatisfaction can be contagious.

#### Identically distributed observations

To check the assumption of identically distributed observations, we inspect the residuals. Remember, the residuals are represented by the error term ($e$) in the regression equation. They are the difference between the scores that we observe for our respondents and the scores that we predict for them with our regression model.

```{r resid-normal, fig.cap="What are the residuals and how are they distributed?", echo=FALSE, screenshot.opts = list(delay = 5), dev="png", out.width="420px"}
# Goal: Understand the meaning of residuals by linking residuals in a
# scatterplot to the x values in a histogram.
# Generate a sample (N = 20?) with a weak negative effect (-0.6) of exposure on
# attitude and exposure, with a sizable error term to have residuals that are
# clearly visible. Generate a sample with uniformly distributed residuals.
# Display the sample as a scatterplot with the regression line and (red) line
# segments linking the dots vertically to the regression line. Display the
# residuals also as a histogram with normal curve. Hovering over/clicking a line
# segment (residual) in the scatterplot should highlight the corresponding bar
# in the histogram. Add a button to draw a new sample.
knitr::include_app("http://82.196.4.233:3838/apps/resid-normal/", height="480px")
```

1. What do the lines between dots and regression line represent in the scatter plot of Figure \@ref(fig:resid-normal)?

```{r eval=FALSE}
* A red line depicts the residual or prediction error: The difference between
the actual outcome score (attitude) for a respondent (dot) and the outcome
score predicted by the regression line (blue) for the predictor score
(exposure) of the respondent.
```

2. What is the relation between the scatter plot and the histogram? Can you point out the dot in the scatter plot that belongs to the leftmost bar in the histogram?

```{r eval=FALSE}
* The residuals represented by the red lines in the scatterplot are counted in
the histogram.
* Drag the mouse pointer around one or more dots in the scatterplot while
pressing the left mouse button to select on or more observations (dots). You
will see where they are featured in the histogram (blue). The dot that is
furthest below the regression line is counted in the left-most bar of the
histogram.
```

3. Draw some new samples. Are the residuals always normally distributed?

```{r eval=FALSE}
* No, sometimes the histogram is clearly skewed (asymmetrical).
```

If we sample from a population where attitude towards smoking depends on exposure, smoking status, and contact with smokers, we will be able to predict attitude from the predictors in our sample. Our predictions will not be perfect, sometimes too high and sometimes to low. The differences between predicted and observed attitude scores are the residuals. 

If our sample is truly a random sample with independent and identically distributed observations, our predictions should be equally bad or equally well for each value of the outcome variable, that is, attitude in our example. More specifically, the sizes of our errors (residuals) should be normally distributed for each attitude level (according to the central limit theorem). 

So for all possible values of the outcome variable, we must collect the residuals for the observations that have this score on the outcome variable. For example, we should select all respondents who score 4.5 on the attitude towards smoking scale. Then, we select the residuals for these respondents and see whether they are approximately normally distributed.

Usually, we do not have more than one observation (if any) for each single outcome score, so we cannot practically apply this check. Instead, we use a simple and coarse approach: Are all residuals normally distributed?

A histogram with an added normal curve helps us to evaluate the distribution of the residuals. If the curve more or less follows the histogram, we conclude that the assumption of identically distributed observations is plausible. If not, we conclude that the assumption is not plausible and we warn the reader that the results can be biased.

#### Linearity and prediction errors

The regression models that we discuss assume that the association between the predictor and outcome variables is linear. Residuals of the regression model help us to see whether this is the case.

```{r pred-linearity, fig.cap="How do residuals tell us whether the relation is linear?", echo=FALSE, screenshot.opts = list(delay = 5), dev="png", out.width="430px"}
# Goal: Understand the relation between linear model (scatterplot) and residuals plot by manipulating the shape of the association.
# Generate a sample (N = 20?) with a weak negative effect (-0.6) of exposure on attitude, with a sizable error term to have residuals that are clearly visible. Generate either a sample with (1) linear, (2) curved, (3) U-shaped association. Display the sample as a scatterplot with the regression line and (red) line segments linking the dots vertically to the regression line. Display the residuals also in a residuals (Y) by predicted values (X) plot. Hovering over/clicking a dot in the scatterplot should highlight the corresponding dot in the residuals plot. Add a button to select a different association shape. Upon selection of a shape, generate & display new sample data.
knitr::include_app("http://82.196.4.233:3838/apps/pred-linearity/", height="550px")

#ADD OPTION: LINEAR POSITIVE?
```

1. Which dot in the plot of residuals (Figure \@ref(fig:pred-linearity) bottom) corresponds with the left-most observation (dot) in the scatter plot of attitude by exposure (Figure \@ref(fig:pred-linearity) top)? Drag your mouse around the left-most dot while pressing the left mouse button to check your choice. Repeat for more dots until you understand the relation between the two plots.

```{r eval=FALSE}
* The blue regression line in the top graph represents the predicted values on
the attitude variable from exposure scores. The predicted attitude value for
the left-most observation is the value at which the blue line intersects with
the vertical red line dropping down from that observation.
* The left-most observation is the highest predicted value because the
regression line slopes down to the right. The highest predicted value is at
the far right of the residuals by predicted attitude graph (bottom graph)
because the predicted values are on the horizontal axis of this graph. The
left-most observation in the top graph, then, must correspond with the
right-most observation in the bottom graph.
* Note that this is not always true. If the regression slope is positive, that
is, the regression line goes up from left to right, the left-most observation
in the top graph has the lowest predicted value, so it corresponds to the
left-most observation in the residuals plot.
```

2. Select a U-shaped curve in Figure \@ref(fig:pred-linearity). Explain how the plot of residuals tells you that the association is not linear. Do the same for a curved association.

```{r eval=FALSE}
* If the association between two variables is U-shaped, the regression line
underestimates the attitude for observations with low exposure, overestimates
the attitude for medium values of exposure, and underestimates the attitude
for high exposure values. As a result, the residuals have a marked pattern
from left to right: a set of positive residuals, followed by a set of negative
residuals, followed by a set of positive residuals.
* The same phenomenon occurs for a curved association.
* In contrast, a linear association yields residuals without a clear pattern.
At all levels of exposure and, hence, at all predicted levels of attitude, we
may encounter both positive and negative residuals. In the residuals by
predicted values plot, we have positive and negative residuals everywhere. We
may underestimate as well as overestimate the outcome variable everywhere.
```

The other two assumptions that we use tell us about problems in our model rather than problems in our statistical inferences. Our linear regression model assumes a linear effect of the predictors on the outcome variable (_linearity_) and it assumes that we can predict the outcome equally well or equally badly for all levels of the outcome variable (_homoscedasticity_). 

The regression models that we estimate assume a linear model. This means that an additional unit of the predictor always increases the predicted value by the same amount. If our regression coefficient for the effect of exposure on attitude is -0.25, an exposure score of one predicts a 0.25 more negative attitude towards smoking than zero exposure. Exposure score five predicts the same difference in attitude than score four, exposure score 10 predicts the same difference in comparison to exposure score nine, and so on. As a consequence of the linearity assumption, we can graph a regression model as a straight line.

The relation between a predictor and outcome variable, for example, exposure and attitude towards smoking, need not be linear. It can be curved or have some other fancy shape. Then, the linearity assumption is not met. A straight regression line does not nicely fit the data.

We can see this in a graph showing the (standardized) residuals (vertical axis) against the (standardized) predicted values of the outcome variable (on the horizontal axis). Note that the residuals represent prediction errors. If our regression predictions are systematically too low at some levels of the outcome variable and too high at other levels, the residuals are not nicely distributed around zero for all predicted levels of the outcome variable. This is what you see if the association is curved or U-shaped.

This indicates that our linear model does not fit the data. If it would fit, the average prediction error is zero for all predicted outcome levels. Graphically speaking, our linear model matches the data if positive prediction errors (residuals) are more or less balanced by negative prediction errors everywhere along the regression line.

#### Homoscedasticity and prediction errors

The plot of residuals by predicted values of the outcome variable tells us more than whether a linear model fits the data.

```{r pred-homoscedasticity, fig.cap="How do residuals tell us that we predict all values equally well?", echo=FALSE, screenshot.opts = list(delay = 5), dev="png", out.width="430px"}
# Goal: Understand the relation between linear model (scatterplot) and residuals plot by manipulating homoscedasticity.
# Generate a sample (N = 20) with a weak negative effect (-0.6) of exposure on attitude and exposure, with a sizable error term to have residuals that are clearly visible. Generate error terms with a dependency on the predictor ranging from -1 to +1. Display the sample as a scatterplot with the regression line and (red) line segments linking the dots vertically to the regression line. Display the residuals also in a residuals (Y) by predicted values (X) plot. Hovering over/clicking a dot in the scatterplot should highlight the corresponding dot in the residuals plot. Add a slider (range [-@, 0], initial value 0) to set the levl of heteroscedasticity. Upon slider change, generate & display new sample data.
knitr::include_app("http://82.196.4.233:3838/apps/pred-homoscedasticity/", height="550px")
```

1. What strikes you about the residuals in Figure \@ref(fig:pred-homoscedasticity)?

```{r, eval=FALSE}
* The residuals are higher for higher values of exposure.
* In the top graph, the residuals tend to become larger from left to right:
The residuals are larger for larger exposure scores (horizontal axis in the
top graph).
* In this particular example, larger exposure scores predict more negative
('lower') attitudes towards smoking (vertical axis in the top graph). We can
predict high attitude levels better (smaller residuals) than low attitude
levels (larger residuals).
* Attitudes are on the horizontal axis in the bottom graph, so the residuals
tend to become smaller if we go from left (low attitude scores) to right (high
attitude scores).
* The regression model seems to predict attitude better for participants with low
exposure scores than for participants with high exposure scores.
```

2. What happens if you move the slider to the far left?

```{r, eval=FALSE}
* The pattern reverses. Now, residuals are larger for low values of exposure
or, equivalently in this model with a negative slope, for higher predicted
values of attitude.
* In this situation, we are better at predicting low attitude levels than high
attitude levels. Note that we prefer to predict all attitude levels equally
well.
```

3. At which slider position are all attitude levels predicted equally well or equally badly?

```{r eval=FALSE}
* If the slider is positioned at or around zero, the residuals are more or
less equal for low, medium, or high predicted values of attitude.
Here, we can predict all levels of attitude equally well or equally badly.
* This is best seen in the bottom graph: The vertical spread of observations
is more or less the same at the left, middle, and right of the graph. The
vertical diameter of the dot cloud is more or less equal from left to right.
* This is how we like the plot to look.
```

The other assumption states that we can predict the outcome variable equally well at all outcome variable levels. In other words, the prediction errors (residuals) are more or less the same at all levels of the outcome variable. If we have large prediction errors at some levels of the outcome variable, we should also have large prediction errors at other levels. As a result, the vertical width of the residuals by predictions scatter plot should be more or less the same from left to right. 

If the prediction errors are not more or less equal for all levels of the predicted outcome, our model is better at predicting some values than other values. For example, low values can be predicted better than high values of the outcome variable. This may signal, among other things, that we need to include moderation in the model.


```{r eval=FALSE, echo=FALSE}
#DROPPED

Why do we use the residuals and predicted values instead of a scatterplot for each outcome-predictor variable pair to assess linearity and homoscedasticity? The reason is that some predictors may predict low outcome values and other predictors may predict high outcome values. This is perfectly OK if together they predict low and high outcome values equally well.
```

### Visualizing predictions

A line in a scatterplot is a nice visualization of a regression model. The regression line represents the predicted values of the outcome variable for all values of the predictor. 

In a simple regression model, we have just one predictor, so we have only one regression line. In a multiple regression model, we can draw a lot of regression lines even if we use the same predictor.

```{r regression-predict, fig.cap="How do predictions based on exposure depend on values of smoking status and smoker contact?", echo=FALSE, out.width="580px", screenshot.opts = list(delay = 5), dev="png"}
# Goal: sensitize student to the notion that prediction in a multiple regression
# model requires selecting one predictor and fixed values for the other
# predictors. Without moderation, the fixed values only change the vertical
# position of the line (that is, the constant) but not the slope.
# Generate a data set with attitude towards smoking as Y and exposure as X with a negative (b = -0.6) more or less linear relation and a regression line for the currently selected values of the covariates (smoking status, initial value is 0) and contact (initial value is 3)): attitude = 0.25 - 0.6 * exposure + 0.50 * smoker +  0.15 * contact
# Display the scatterplot.
# Add the line of a simple regression of attitude on exposure for the generated data in grey.
# Add the multiple regression line.
# Display the multiple regression equation: (see above). 'smoker' is a selection box with two options: 0 = non-smoker, 1 = smoker. 'contact' is followed by a numerical input that accepts values between 0 and 10.
# A change to the smoker selection or contact value triggers the app to redraw the regression line for attitude by exposure for these values of smoking status and (smoker)contact.
knitr::include_app("http://82.196.4.233:3838/apps/regression-predict/", height="620px")
```

1. What happens in Figure \@ref(fig:regression-predict) if you change smoking status or smoking contact score? Can you explain the size of changes?

```{r eval=FALSE}
* The (red) multiple regression line moves up or down.
* The change from non-smoker to smoker adds 0.5 to the predicted values. The
red line moves up 0.5 points on the attitude scale.
* An additional unit of contact adds 0.15 to the predicted values. The red line
moves up 0.15 points on the attitude scale.
```

2. The line for a simple regression of attitude on exposure (with just one predictor) is displayed in blue in Figure \@ref(fig:regression-predict). Can you make the multiple regression line equal to the simple regression line? If so, at what covariate values? If not, why not?

```{r eval=FALSE}
* It is possible to have the red regression line fully overlap the blue
regression line because they have the same slope, namely -0.6. The multiple
and simple regression lines are parallel. If we adjust the values of smoking
status or contact with smokers, we can move the red line on top of the blue
line.
* For non-smokers, we have to set contact near six for the red line to overlap
the blue one. The contact value for smokers is near 3.
* If you like, you can calculate the exact values for contact at which the
multiple and simple regression lines are equal. Link the two equations and plug
in the values that you know. For smokers:
* 1.575 - 0.6 * Exposure = 0.65 - 0.6 * Exposure + 0.5 * Smoker + 0.15 * Contact
* Exposure drops from the equation because it occurs both left and right. The
value for smokers is 1 on Smoker.
* 1.575 = 0.65  + 0.5 * 1 + 0.15 * Contact
* Rearrange:
* 1.575 - 0.65 - 0.5 = 0.15 * Contact
* Divide left and right by 0.15:
* (1.575 - 0.65 - 0.5) / 0.15 = Contact
* 0.425 / 0.15 = Contact
* Contact = 2.833.
```

The regression equation without the error term $e$ predicts the outcome variable scores from the predictor scores. Plug in values for the predictor variables and you can calculate the predicted outcome score. Figure \@ref(fig:regression-predict), for instance, shows a regression equation for the effects of exposure, smoking status, and contact with smokers on attitude towards smoking. If you plug in a score of 4 for exposure, 0 for smoking status (non-smoker), and 6 for contact with smokers, the predicted attitude is 0.65 + -0.6 * 4 + 0.5 * 0 + 0.15 * 6 = -0.85.

If we focus on the relation between one predictor and the outcome variable, the predicted values can be represented by a straight line in a scatter plot. By convention, we display the predictor on the horizontal axis and the outcome variable on the vertical axis of the scatter plot. 

In a simple regression, we only have one predictor, so we can only draw one regression line. In multiple regression, however, we have several predictors. We can draw regression lines for each predictor and, importantly, we can draw different regression lines for the same predictor.

If we want to draw the regression line for the predictive effect of one predictor in a multiple regression model, we regard the other predictors as covariates. Let us define a _covariate_ as a variable that may predict the outcome but is not our prime interest, so we mainly want to control for its effects. For example, if we focus on the predictive effect of exposure on attitude towards smoking, exposure is our predictor and smoking status and contact with smokers are covariates. 

Note that the distinction between predictor and covariates is temporary. As soon as we focus on another variable, that variable becomes the predictor and the other predictors become covariates. The distinction between predictor and covariate is just terminology to show on which variable we focus.

To draw the regression line for the effect of exposure on attitude towards smoking, we must select a value for the covariates. If we would not do so, we have more than one variable that is allowed to vary but we can only display one predictor on the horizontal axis of our scatter plot.

\begin{equation}
\small
\begin{split}
  attitude &= 0.65 + -0.6*exposure + 0.5*status + 0.15*contact \\ 
  attitude &= 0.65 + -0.6*exposure + 0.5*0 + 0.15*3 \\ 
  attitude &= 0.65 + -0.6*exposure + 0 + 0.45 \\ 
  attitude &= 1.10 + -0.6*exposure 
\end{split}
  (\#eq:regsimpleslope) 
\normalsize
\end{equation}

Let us select non-smokers (_status_ equals 0) who score 3 on _contact_ in Equation \@ref(eq:regsimpleslope). If we plug in these values in the regression equation, we obtain a simple regression---just one predictor, namely _exposure_---with a higher constant: 1.10 instead of 0.65. The constant is the intercept of the regression line, that is, the value of the vertical axis where the regression line crosses it. 

The slope of the regression line, however, does not change no matter which values we select for the covariates. The regression coefficient of _exposure_ remains -0.6. The regression line only moves up or down if we choose different values for the covariates. To visualize the exposure effect, it does not matter which values we chose for the covariates. A popular choice is using their average scores. When we add a moderator, however, the slope also changes as we will see in Section \@ref(categoricalmoderator).

## Regression Analysis in SPSS {#SPSS_regression}

### Instructions

```{r SPSSregsimplerepeat, echo=FALSE, out.width="640px", fig.cap="(ref:regsimpleSPSS)", dev="png", screenshot.opts = list(delay = 5)}
knitr::include_url("https://www.youtube.com/embed/qgOLWulSERs", height = "360px")
# Goal: asymmetric association as prediction
# Example: consumers.sav, brand awareness by advertisement exposure
# Technique: regression with confidence intervals
# SPSS menu: regression>linear with CI under Statistics.
# Paste & Run.
# Interpret output: R2, F test, predictive effect strength (b*) and change (b) with 95% confidence interval.
# Check assumptions: in chapter on moderation with regression analysis?
```

----

```{r SPSSregdummy2, echo=FALSE, out.width="640px", fig.cap="(ref:regdummy2SPSS)", dev="png", screenshot.opts = list(delay = 5)}
knitr::include_url("https://www.youtube.com/embed/0y0q-lKiPYk", height = "360px")
# Creating dummy variables in SPSS.
# Goal: Understand creating dummy variables.
# Example: smokers.sav, respondent's smoking status ( 3 categories).
# SPSS menu: Transform > Create Dummy Variables or Trasform > Recode into Different Variables.
# Inspect results: new variables, coded 0/1.
```

----

```{r SPSSregdummy, echo=FALSE, out.width="640px", fig.cap="(ref:regdummySPSS)", dev="png", screenshot.opts = list(delay = 5)}
knitr::include_url("https://www.youtube.com/embed/AJ88dheUieY", height = "360px")
# Using dummy variables in a regression model in SPSS.
# Example: smokers.sav, predict the attitude towards smoking from exposure to an anti-smoking campaign and respondent's smoking status (dummies).
# SPSS menu: Transform > Create Dummy Variables
# Remember: Leave one dummy variable out.
# Interpret results: unstandardized regression coefficient as average difference with reference category. Don't interpret the standardized regression coefficient.
```

----

```{r SPSSreglines1, echo=FALSE, out.width="640px", fig.cap="(ref:reglines1SPSS)", dev="png", screenshot.opts = list(delay = 5)}
knitr::include_url("https://www.youtube.com/embed/OQEylRkeVzI", height = "360px")
# Goal: Adding regression lines to a scattergram for a simple regression (Chart Editor: Add fit line at total), for one predictor in multiple regression for reference values of anther predictor - first one line, then two lines (Chart Editor: reference line from equation).
# Example: smokers.sav, predict the attitude towards smoking from exposure to an anti-smoking campaign and respondent's smoking status.
# SPSS menu: spell out the equation in notepad, Graph > Legacy Dialogs > Scatter/Dot, set markers by status3, Chart Editor, add a reference line from equation, just enter the equation with plugged in values, don't simplify ; colour lines according to the smoking status group
# Interpret output: vertical difference between lines is average difference between categories, equals unstandardized regression coefficient.
```

----

```{r SPSSregassumpt, echo=FALSE, out.width="640px", fig.cap="(ref:regassumptSPSS)", dev="png", screenshot.opts = list(delay = 5)}
knitr::include_url("https://www.youtube.com/embed/AtzXHORzxlA", height = "360px")
# Goal: Inspecting residuals.  (see Chapter 4 on hypothesis testing for video about regression basics.)
# Example: smokers.sav, predict the attitude towards smoking from exposure to an anti-smoking campaign and respondent's smoking status.
# Technique: regression analysis
# SPSS menu: linear regression, add plots
# Interpret output = check assumptions: Chart Editor of zresid * zpred plot: add reference line at 0 and perhaps at +2 and -2 to inspect shape of residual distribution.
```

### Exercises

1. Use the data in [smokers.sav](http://82.196.4.233:3838/data/smokers.sav) to predict the attitude towards smoking from exposure to an anti-smoking campaign. Check the assumptions and interpret the results.

```{r eval=FALSE}
SPSS syntax:  
  
* Check data.  
FREQUENCIES VARIABLES=exposure attitude  
  /ORDER=ANALYSIS.  
* Simple regression analysis with assumption checks.  
REGRESSION  
  /MISSING LISTWISE  
  /STATISTICS COEFF OUTS CI(95) R ANOVA  
  /CRITERIA=PIN(.05) POUT(.10)  
  /NOORIGIN   
  /DEPENDENT attitude  
  /METHOD=ENTER exposure  
  /SCATTERPLOT=(*ZRESID ,*ZPRED)  
  /RESIDUALS HISTOGRAM(ZRESID).  
  
Check data:  
  
There are no impossible values on the two variables.  
  
Check assumptions:  
  
* The distribution of the residuals is (left) skewed rather than normal. This
is not good.
* The residuals are nicely centered around zero at all predicted levels of the
outcome variable, so the association seems to be linear.
* The residuals are evenly spread around zero at all predicted levels.
Perhaps, the spread is slightly larger at low predicted values. In all,
however, prediction accuracy seems to be more or less the same at all levels
(homoscedasticity).
  
Interpret the results:  
  
* Campaign exposure predicts attitude towards smoking among adults reasonably
well, R2 = .17, F (1, 83) = 16.73, p < .001.
* One additional unit of exposure decreases the predicted attitude by 0.12 to
0.34 points, t = -4.09, p < .001, 95%CI[-0.34; -0.12]. This is a moderate to
strong effect (b* = -.41).
```

2. Add smoking status (variable _status3_), and contact with smokers as predictors to the regression model of Exercise 1. Compare the effect of exposure between the two regression models. What is the difference and why is there a difference?

```{r eval=FALSE}
SPSS syntax:  
  
* Check data.  
FREQUENCIES VARIABLES=exposure status3 contact attitude  
  /ORDER=ANALYSIS.  

* Create dummy variables for status3.  
* With Transform > Create Dummy Variables. 
* ENSURE THAT MEASUREMENT LEVEL I SET TO ORDINAL.  
* Define Variable Properties.  
*status3.  
VARIABLE LEVEL  status3(ORDINAL).  
EXECUTE.  
SPSSINC CREATE DUMMIES VARIABLE=status3   
ROOTNAME1=status   
/OPTIONS ORDER=A USEVALUELABELS=YES USEML=YES OMITFIRST=NO.  

* If your SPSS version does not have this command, user Recode.
RECODE status3 (1=1) (ELSE=0) INTO status_2.
VARIABLE LABELS  status_2 'Former smoker'.
EXECUTE.
RECODE status3 (2=1) (ELSE=0) INTO status_3.
VARIABLE LABELS  status_3 'Smoker'.
EXECUTE.

* Multiple regression analysis with assumption checks.  
REGRESSION  
  /MISSING LISTWISE  
  /STATISTICS COEFF OUTS CI(95) R ANOVA  
  /CRITERIA=PIN(.05) POUT(.10)  
  /NOORIGIN   
  /DEPENDENT attitude  
  /METHOD=ENTER exposure status_2 status_3 contact  
  /SCATTERPLOT=(*ZRESID ,*ZPRED)  
  /RESIDUALS HISTOGRAM(ZRESID).  
  
Check data:  
  
All values on the variables seem to be valid.  
  
Check assumptions:  
  
* The residuals seem to be a little skewed rather than symmetrical as in a
normal distribution.
* The residuals by predicted values plot is not as it should be. At low
predicted levels, the residuals are below zero, so the average residual is not
zero. The associations do not seem to be linear.
* In addition, the spread of the residuals is larger at higher predicted
levels (right) than low predicted levels (left). This pattern suggests that
the effects are moderated (see next section).
  
Interpret the results:  
  
* Include a table with regression coefficients, so we need not report all t
test results in our interpretation.
  
* The regression model predicts about sixty per cent of the variation in
attitude towards smoking, which is very much for a social scientific model, R2
= .62, F (4, 80) = 32.02, p < .001.
* Exposure to the anti-smoking campaign predicts a more negative attitude
towards smoking, while contact with smokers is associated with a slightly more
positive attitude. Of the two, exposure (b* = -0.44) is a better predictor
than contact with smokers (b* = 0.20).
* Former smokers are on average much (2.85 points) more negative about smoking
than non-smokers and smokers; smokers are on average only 0.20 points below the
average attitude of non-smokers.
* The residuals suggest that the assumptions for using the theoretical
approximation of the sampling distributions may not have been met and/or that
the model should not be linear.
```

3. The data set [children.sav](http://82.196.4.233:3838/data/children.sav) contains information about media literacy of children and parental supervision of their media use. Are the two related? Check the assumptions and interpret the results.

```{r eval=FALSE}
* The analysis method you choose, depends on your substantive decision on the
direction of the association.
* If you assume no direction, a correlation coefficient is the most
appropriate choice. If you think one variable may depend on another, a
(simple) regression model is the best choice.
  
SPSS syntax:  
  
* Check data.  
FREQUENCIES VARIABLES=medliter supervision  
  /ORDER=ANALYSIS.  
* Set supervision 25 to missing.  
* Define Variable Properties.  
*supervision.  
MISSING VALUES supervision(25.00).  
EXECUTE.  
* Undirected: correlation (linear?).  
* Check scatterplot.  
GRAPH  
  /SCATTERPLOT(BIVAR)=supervision WITH medliter  
  /MISSING=LISTWISE.  
* Correlations.  
CORRELATIONS  
  /VARIABLES=medliter supervision  
  /PRINT=TWOTAIL NOSIG  
  /MISSING=PAIRWISE.  
* Simple regression: media literacy dependent.  
REGRESSION  
  /MISSING LISTWISE  
  /STATISTICS COEFF OUTS CI(95) R ANOVA  
  /CRITERIA=PIN(.05) POUT(.10)  
  /NOORIGIN   
  /DEPENDENT medliter  
  /METHOD=ENTER supervision  
  /SCATTERPLOT=(*ZRESID ,*ZPRED)  
  /RESIDUALS HISTOGRAM(ZRESID).  
* Simple regression: parental supervision dependent.  
REGRESSION  
  /MISSING LISTWISE  
  /STATISTICS COEFF OUTS CI(95) R ANOVA  
  /CRITERIA=PIN(.05) POUT(.10)  
  /NOORIGIN   
  /DEPENDENT supervision  
  /METHOD=ENTER medliter  
  /SCATTERPLOT=(*ZRESID ,*ZPRED)  
  /RESIDUALS HISTOGRAM(ZRESID).  
  
Check data:  
  
Score '25' for parental supervision cannot be right because the scale runs to
10. Define this score as a missing value.
  
Check assumptions:  
  
* In the regression models, the residuals are quite normally distributed,
nicely grouped around zero at all levels of the predicted outcome.
* The variation of residuals is more or less the same at different levels of
the predicted outcome but there are perhaps too few observations for low and
high predicted levels to decide.
  
Interpret the results:  
  
Parental supervision and child media literacy are weakly correlated (r = .36,
p = .001). More supervision predicts more media literacy, t = 3.54,
p = .001, 95%CI[0.13; 0.48]. One additional unit of parental supervision
predicts 0.31 additional units of media literacy. One additional unit of media
literacy predicts 0.42 additional units of parental supervision.
  
Note that the t test on the regression coefficient is the same in a simple
regression model if you use supervision or media literacy as outcome variable.
```

## Different Lines for Different Groups {#categoricalmoderator}

If we have a categorical independent variable, for instance, people living among smokers versus people living among non-smokers, and we want to determine its effect on a numerical variable, for example, attitude towards smoking, we compare group means. The difference between group means is the main effect of the categorical variable. For example, the average attitude towards smoking is 0.5 points more positive for people living among smokers than for people who do not live among smokers. 

In analysis of variance (Chapter \@ref(anova)), the main effect of living among smokers is the average effect for all people regardless of their other characteristics or the contexts that they are in. In other words, a main effect is the overall difference in attitude between people living among smokers and those who are not living among smokers.

What if the effect of living among smokers on attitude may be different in different contexts, e.g., for people who smoke themselves, used to smoke, or never smoked (*smoking status*)? To model this, we added an interaction effect to the main effects in Chapter \@ref(anova).  

The interaction effect tells us whether the attitude difference between, on the one hand, people living among smokers and, on the other hand, people living among non-smokers is different for smokers, former smokers, and non-smokers. In a conceptual diagram, the interaction effect is represented by an arrow pointing at another arrow. The moderator (smoking status) changes the relation between the predictor (living among smokers) and the outcome (attitude towards smoking).

```{r moderator-concept2, echo=FALSE, fig.cap="Conceptual diagram of moderation.", fig.asp=0.3}
library(ggplot2)
# Create coordinates for the variable names.
variables <- data.frame(x = c(0.3, 0.5, 0.7), 
                        y = c(.1, .3, .1),
                        label = c("Predictor", "Moderator", "Outcome"))
ggplot(variables, aes(x, y)) + 
  geom_segment(aes(x = variables$x[1], y = variables$y[1], xend = variables$x[3] - 0.04, yend = variables$y[1]), arrow = arrow(length = unit(0.04, "npc"), type = "closed")) + 
  geom_segment(aes(x = variables$x[2], y = variables$y[2], xend = variables$x[2], yend = variables$y[1]), arrow = arrow(length = unit(0.04, "npc"), type = "closed")) +
  geom_label(aes(label=label)) + 
  coord_cartesian(xlim = c(0.2, 0.8), ylim = c(0, 0.4)) +
  theme_void()
# Cleanup.
rm(variables)
```

What if living among smokers is a numerical variable, for example, counting the number of times the respondent is in the company of a smoker? How can we investigate if the effect of this numerical predictor on attitude is moderated by the respondent's smoking status?

Analysis of variance (ANOVA), as discussed in Chapter \@ref(anova), investigates the effects of categorical variables on a numeric outcome variable. It cannot handle numeric predictors or numeric covariates. Although there are ways to include numeric covariates in analysis of variance, for instance, in the analysis of covariance (ANCOVA), we use regression analysis if we have at least one numerical predictor or covariate and a numerical outcome. 

In the current section, we discuss regression models with a numerical predictor and a categorical moderator. A later section (Section \@ref(cont-moderator-regression)), presents regression models in which both the predictor and moderator are numeric.

### A dichotomous moderator and continuous predictor

```{r dichotomous-moderator, fig.cap="Is the effect of exposure on attitude moderated by smoking status?", echo=FALSE, screenshot.opts = list(delay = 5), dev="png", out.width="420px"}
# Goal: Sensitize the student to the notion that moderation in a regression
# model means different slopes for different groups.
# In a graph with attitude as Y axis and exposure as X axis, generate two regression lines, one for smokers and one for non-smokers. 
# Plot the regression lines in a scatterplot. 
# Show the regression equation for each line (preferably in the plot). 
# Systematically vary the slopes (the same, one more negative than the other,
# opposite signs) and the constant difference (positive, nearly zero,
# negative).
# Add a Generate New button to replace the regression lines by a new pair of lines.
knitr::include_app("http://82.196.4.233:3838/apps/dichotomous-moderator/", height="350px")
```

1. Is the effect of exposure on attitude moderated by the smoking status of respondents (smokers versus non-smokers) in Figure \@ref(fig:dichotomous-moderator)? Motivate your answer. Press the *Take new sample* button to practice some more with recognizing moderation.

```{r eval=FALSE}
* Remember: The slope of the regression line represents the effect (the
regression coefficient) of the predictor. Parallel lines mean equal slopes.
* The answer depends on the sample drawn. If the regression lines for the two
smoking status groups are more or less parallel, the (predictive) effect of
exposure on attitude is more or less the same for both groups.
* In this case, there is no moderation.
* In all other situations, there is moderation.
```

In Section \@ref(regression-equation), we have analyzed the predictive effects of exposure to an anti-smoking campaign and smoking status on a person's attitude towards smoking. We have found a negative effect for exposure and a positive effect for smoking. More exposure predicts a more negative attitude whereas smokers have a more positive attitude towards smoking than non-smokers.

Our current question is: Does exposure to the campaign have the same effect for smokers and non-smokers? We want to compare an effect (exposure on attitude) for different contexts (smokers versus non-smokers), so our current question involves moderation. Is the effect of exposure on attitude moderated by smoking status?

Our moderator (smoker vs. non-smoker) is a dichotomous variable but our predictor (exposure) is numeric, so we cannot use analysis of variance. Instead, we use regression analysis, which allows numeric predictors. 

In the context of a regression model, moderation means __different slopes for different groups__. The slope of the regression line is the regression coefficient, which expresses the effect of the predictor on the outcome variable. If we have different effects in different contexts (moderation), we must have different regression coefficients for different groups.

### Interaction variable {#interaction-variable}

```{r interaction-var-effect, eval=FALSE, echo=FALSE, fig.cap="What does an interaction variable do?"}
# Goal: Intuitive understanding of the effect of an interaction variable.
# Generate a dataset with 30 observations for the regression model y = 3 -
# 0.5x_1 + 1.5x_2 + 0.3x_1*x_2 with x_1 in the range [0, 10] and x_2 a dummy (0
# or 1) with a random uniform component to each parameter in the range [-.1,
# .1].
# In a scatterplot of attitude (Y) versus exposure (X), display the regression
# line (fat, grey) for the equation with x_2 = 0, labelled with the regression
# equation without the interaction variable. Display smokers and non-smokers
# with different colours/shapes.
# Add a select list labeled 'Add product of exposure and smoking status' with
# the values '--', '0 - Non-smokers', and '1 - Smokers'. Selection of a value
# adds the corresponding regression line to the plot with the category name and
# regression equation. (The line for non-smokers is parallel to the fat gray
# line.)
# Clicking/hovering over the newly created line shows the slope as a sum of the
# conditional and interaction effect, e.g., "Slope: 0.5 * exposure + 0.3 * 1 *
# exposure".

1. In Figure \@ref(fig:interaction-var-effect), does the fat grey line represent the effect of exposure on attitude in a simple regression model, a multiple regression model, or both?

2. Select an option under "Add product" and explain what the newly created regression line means.

3. Select the other option. Explain why the two regression lines that you created have different slopes.
```

How do we obtain different regression coefficients and lines for smokers and non-smokers? The statistical trick is quite easy: Include a new predictor in the model that is the product of the predictor (exposure) and the moderator (smoking status). This new predictor is the _interaction variable_. It must be included together with the original predictor and moderator variables, see Equation \@ref(eq:intvar). This is also visible in the statistical diagram (Figure \@ref(fig:moderator-statistical)) for moderation in a regression model.

\begin{equation}
\small
\begin{split}
  attitude = &\ constant + b_1*exposure + b_2*smoker + b_3*contact\\
  &+ b_4*exposure*smoker + e 
\end{split}
(\#eq:intvar) 
\normalsize
\end{equation}

```{r moderator-statistical, fig.cap="Statistical diagram of moderation.", echo=FALSE, fig.asp=0.4}
library(ggplot2)
# Create coordinates for the variable names.
variables <- data.frame(x = c(rep.int(0.3, times = 4), 0.7), 
                        y = c(.4, .3, .2, .1, .25),
                        label = c("Exposure", "Smoker", "Contact", "Exposure*Smoker", "Attitude"))
# Add coordinates for arrow endpoint.
x_diff <- 0.04
variables$xend <- variables$x[5] - x_diff #fixed translation to the left
variables$yend <- variables$y[5] + x_diff * (variables$y - variables$y[5]) / (variables$x[5] - variables$x)
rm(x_diff)
ggplot(variables, aes(x, y)) + 
  geom_segment(aes(x = variables$x[1], y = variables$y[1], xend = variables$xend[1], yend = variables$yend[1]), arrow = arrow(length = unit(0.04, "npc"), type = "closed")) + 
  geom_segment(aes(x = variables$x[2], y = variables$y[2], xend = variables$xend[2], yend = variables$yend[2]), arrow = arrow(length = unit(0.04, "npc"), type = "closed")) + 
  geom_segment(aes(x = variables$x[3], y = variables$y[3], xend = variables$xend[3], yend = variables$yend[3]), arrow = arrow(length = unit(0.04, "npc"), type = "closed")) + 
  geom_segment(aes(x = variables$x[4], y = variables$y[4], xend = variables$xend[4], yend = variables$yend[4]), arrow = arrow(length = unit(0.04, "npc"), type = "closed")) + 
  geom_label(aes(label=label)) + 
  coord_cartesian(xlim = c(0.2, 0.8), ylim = c(0.1, 0.5)) +
  theme_void()
```

The smoking status variable is coded 1 for smokers and 0 for non-smokers. For clarity, we name this variable smoker with score 1 for Yes and score 0 for No. Remember (Section \@ref(dichpredictor)) that we have two different regression equations, one for each group on the dichotomous predictor _status_. Just plug in the two possible values (1 and 0) for this variable. For non-smokers, the interaction variable drops from the model because multiplying with zero yields zero. For non-smokers, our reference group, $b_1$ represents the effect of exposure on attitude. It is called the _simple slope_ of exposure for non-smokers.

\begin{equation}
\small
\begin{split}
  attitude = &\ constant + b_1*exposure + b_2*smoker + b_3*contact\\
    &+ b_4*exposure*smoker + e \\
  attitude = &\ constant + b_1*exposure + b_2*0 + b_3*contact\\
    &+ b_4*exposure*0 + e \\
  attitude = &\ constant + b_1*exposure + b_3*contact + e
\end{split}
(\#eq:intvarnonsmoker) 
\normalsize
\end{equation}

In contrast, the interaction variable remains in the model for smokers, who score 1 on smoking status. Note what happens with the coefficient of the exposure effect if we rearrange the terms a little: The exposure effect equals the effect for the reference group of non-smokers ($b_1$) plus the effect of the interaction variable ($b_4$). The simple slope for smokers, then, is $b_1 + b_4$.

\begin{equation}
\small
\begin{split}
  attitude = &\ constant + b_1*exposure + b_2*smoker + b_3*contact\\
  &+ b_4*exposure*smoker + e \\
  attitude = &\ constant + b_1*exposure + b_2*1 + b_3*contact\\
  &+ b_4*exposure*1 + e \\
  attitude = &\ constant + b_1*exposure + b_4*exposure + b_2 + b_3*contact + e \\
  attitude = &\ constant + (b_1 + b_4)*exposure + b_2 + b_3*contact + e 
\end{split}
(\#eq:intvarsmoker) 
\normalsize
\end{equation}

The interaction effect ($b_4$) shows the difference between the simple slope of the exposure effect for smokers ($b_1+b_4$) and the simple slope for non-smokers ($b_1$). Let us assume that the unstandardized regression coefficient of the interaction effect is -0.3 (see Section \@ref(interactioninterpretation)). This means that the effect of exposure on attitude is more strongly negative (or less positive) for smokers than for non-smokers. One additional unit of exposure decreases the predicted attitude for smokers by 0.3 more than for non-smokers.

### Conditional effects, not main effects {#conditional-effects}

It is very important to note that the effects of exposure and smoking status in a model with exposure by smoking status interaction are __not__ main effects as in analysis of variance. As we have seen in the preceding section (Equation \@ref(eq:intvarnonsmoker)), the regression coefficient $b_1$ for exposure expresses the effect of exposure for the reference group of non-smokers. It is a _conditional effect_, namely the effect for non-smokers only. This is quite different from a main effect, which is an average effect over all groups.

In a similar way, the regression coefficient $b_2$ for smoking status expresses the effect for persons who score zero on the exposure predictor. Simply plug in the value 0 for exposure in the regression equation (Equation \@ref(eq:simplestatus)).

\begin{equation}
\small
\begin{split}
  attitude = &\ constant + b_1*exposure + b_2*smoker + b_3*contact\\
    &+ b_4*exposure*smoker + e \\
  attitude = &\ constant + b_1*0 + b_2*smoker + b_3*contact\\
    &+ b_4*0*smoker + e \\
  attitude = &\ constant + b_2*smoker + b_3*contact + e
\end{split}
(\#eq:simplestatus) 
\normalsize
\end{equation}

Smoking status is a dichotomy, so its regression coefficient ($b_2$) tells us the average difference in attitude between smokers and non-smokers. Due to the inclusion of the interaction variable, it now tells us the difference in average attitude between smokers and non-smokers who have zero exposure to the anti-smoking campaign. Note again that this is a conditional effect, not a main effect.

### Interpretation and statistical inference {#interactioninterpretation}

```{r dich-moderator-output, echo=FALSE, message=FALSE, warning=FALSE}
# Table of regression coefficients for exposure moderated by status. Similar to SPSS output (with correct standardized coefficients).
# Create effect sizes.
smokers <- haven::read_spss("data/smokers.sav")
model_1 <- lm(attitude ~ exposure*status2, data = smokers)
# Table with results in SPSS style.
results <- coef(summary(model_1))
# Adjust parameter names
attributes(results)$dimnames[[1]][1] <- "(Constant)"
attributes(results)$dimnames[[1]][2] <- "Exposure"
attributes(results)$dimnames[[1]][3] <- "Status (smoker)"
attributes(results)$dimnames[[1]][4] <- "Exposure*Status (smoker)"
# Confidence intervals
ci <- confint.lm(model_1)
results <- cbind(results, ci)
# Correctly standardized coefficients.
attach(smokers)
z_exposure <- (exposure - mean(exposure)/sd(exposure))
z_status2 <- (status2 - mean(status2)/sd(status2))
z_expostatus2 <- (exposure * status2 - mean(exposure * status2)/sd(exposure * status2))
z_attitude <- (attitude - mean(attitude)/sd(attitude))
model_2 <- lm(z_attitude ~ z_exposure + z_status2 + z_expostatus2)
results_2 <- coef(summary(model_2))
results <- cbind(results[, 1:2], results_2[, 1], results[, 3:6])
results[1, 3] <- NA
# Set column names.
attributes(results)$dimnames[[2]] <- c("B", "Std. Error", "Beta", "t", "Sig.", "Lower Bound", "Upper Bound")
# Table.
options(knitr.kable.NA = '')
knitr::kable(results, digits = 3, caption = "Predicting attitude towards smoking: regression analysis results.")
# Helper function for displaying results within the text.
source("report_n.R")
#Cleanup (partial).
rm(smokers, model_1, smokers, ci, results_2, model_2, z_attitude, z_status2, z_expostatus2, z_exposure)
```

In Table \@ref(tab:dich-moderator-output), non-smokers are the reference group because they are coded 0 on the _Status_ variable. As a consequence, the regression coefficient for exposure gives us the effect of exposure on smoking attitude for non-smokers. It's value is `r report_n(results[2, 1], digits = 2)`, so an additional unit of exposure predicts a smoking attitude among non-smokers that is `r report_n(abs(results[2, 1]), digits = 2)` points more negative. More exposure to the campaign goes together with a more negative attitude towards smoking for non-smokers. The p value for this effect tests the null hypothesis that the effect is zero in the population. If we reject this null hypothesis, the exposure effect is statistically significant for non-smokers.

The effect of (smoking) status on attitude is conditional on exposure. The regression coefficient for status tells us the difference between smokers and non-smokers who have 0 exposure. So, without exposure to the campaign, smokers are on average `r report_n(results[3, 1], digits = 2)` more positive towards smoking than non-smokers. The p value tests the null hypothesis that the difference is zero for people without exposure to the anti-smoking campaign.

Smokers are coded 1 on the (smoking) status variable, so the regression coefficient for the interaction tells us that the slope of the exposure effect is `r report_n(abs(results[4, 1]), digits = 2)` lower for smokers than for non-smokers. In a preceding paragraph, we have seen that the estimated slope of the exposure effect is `r report_n(results[2, 1], digits = 2)` for non-smokers. We can add the regression coefficient of the interaction variable to obtain the estimated slope for smokers, which is `r report_n(results[2, 1] + results[4, 1], digits = 2)`. Now we can compare the two regression lines for the two groups, which gives good insight into the nature of moderation in this example. A graph of the two regression lines is probably the best way to communicate your results.

```{r dich-moderator-graph, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="The effects of exposure to the anti-smoking campaign on attitude towards smoking among smokers and non-smokers."}
# Graph of regression lines calculated in chunk dich-moderator-output.
# Plot.
ggplot() +
  geom_segment(aes(x = 0, xend = 10, 
                   y = results[1,1], yend = results[1,1] + 10*results[2,1],
                   colour = "Non-smokers"),
               size = 1
               ) +
  geom_segment(aes(x = 0, xend = 10, 
                   y = results[1,1] + results[3,1], 
                   yend = results[1,1] + results[3,1] + 10*(results[2,1] + results[4,1]),
               colour = "Smokers"),
               size = 1
               ) +
  scale_x_continuous(limits = c(0, 10), breaks = c(0, 10),
                     labels = c("No exposure", "Maximum\nexposure")) +
  scale_y_continuous(limits = c(-5, 5), breaks = c(-5, 0, 5), 
                     labels = c("negative", "neutral", "positive")) +
  scale_colour_manual(values = c("Non-smokers" = unname(brewercolors["Blue"]),
                                 "Smokers" = "Black")) + 
  labs(x = "Exposure to the anti-smoking campaign", y = "Attitude towards smoking") +
  #Legend definitions
  guides(colour = guide_legend(title = "Smoking status:"),
             fill = FALSE) + 
  theme_general() + 
  theme(legend.position = "top")
# Cleanup.
rm(results, report_n)
```

The interaction variable is treated as an ordinary predictor in the estimation process, so it receives a confidence interval and a p value. The null hypothesis is that the interaction effect is zero in the population. 

Remember that the regression coefficient for the interaction variable expresses the difference between the slope for the indicated group, e.g., smokers, and the slope for the reference group, e.g., non-smokers. If this difference is zero, as stated by the null hypothesis, the two groups have the same slope, so the effect is not moderated by the group variable.

So we know the confidence intervals and p values of the exposure effect for non-smokers (the regression coefficient for exposure) and for the difference between their exposure effect and the exposure effect for smokers (the regression coefficient for the interaction effect). We do not know, however, the confidence interval and statistical significance of the exposure effect for smokers. We cannot add confidence intervals or p values. 

If you want to know the confidence interval or p value of the exposure effect for smokers, you have to rerun the regression analysis using a different dummy variable for the moderator. You should create a dichotomous variable that assigns the 1 score to non-smokers and an interaction variable created with this dichotomy. The regression coefficient of the exposure effect now expresses the effect for smokers because smokers are the reference group on the new dummy variable. The associated p value and confidence interval apply to the exposure effect for smokers.

Interaction variables are used just like ordinary predictors, so the general assumptions of regression analysis apply. See Section \@ref(regr-inference) for a description of the assumptions and checks.

Let us conclude the interpretation with a warning. The standardized regression coefficients that SPSS reports for interaction effects or effects of predictors that are involved in interaction effects __must not be used__. They are calculated in the wrong way if the regression model includes an interaction variable. As a result, they are misleading.

### A categorical moderator
What if we have three or more groups in our moderator? For example, smoking status measured with three categories: (1) never smoked, (2) have smoked, (3) currently smoking? Does the effect of exposure on attitude vary between people who never smoked, stopped smoking, and are still smoking?

```{r categorical-moderator, fig.cap="When do we have moderation with a categorical moderator?", echo=FALSE, screenshot.opts = list(delay = 5), dev="png", out.width="420px"}
# Goal: Sensitize the student to the notion that moderation in a regression
# model means different slopes for different groups.
# In a graph with attitude as Y axis and exposure as X axis, generate two regression lines, one for smokers and one for non-smokers. 
# Plot the regression lines in a scatterplot. 
# Show the regression equation for each line (preferably in the plot). 
# Systematically vary the slopes (the same, one more negative than the other,
# opposite signs) and the constant difference (positive, nearly zero,
# negative).
# Add a Generate New button to replace the regression lines by a new pair of lines.
knitr::include_app("http://82.196.4.233:3838/apps/categorical-moderator/", height="350px")
```

1. If we have exposure effects within three groups, as in Figure \@ref(fig:categorical-moderator), when do we have an interaction effect (moderation) and when do we not have an interaction effect? Motivate your answer. Press the *Take new sample* button to practice recognizing moderation.

```{r eval=FALSE}
* If the regression lines for all three smoking status groups are more or less
parallel, the (predictive) effect of exposure on attitude is more or less the
same for the three groups. In this case, there is no moderation.
* In all other situations, there is moderation. There is an interaction
effect, for example, if two lines are parallel but the third line is not
parallel to the other two lines.
```

In Section \@ref(categorical-predictor), we learned that we must create dummy variables for all but one groups of a categorical predictor in a regression model. This is what we have to do also for a categorical moderator. If the effect of another predictor, such as exposure, is moderated by a categorical variable, we have to create an interaction variable for each dummy variable in the equation. To create the interaction variables, we multiply the predictor with each of the dummy variable as we have done before. 

```{r categorical-moderator-fig, echo=FALSE, fig.cap="Statistical diagram with a moderator consisting of three groups. Non-smokers are the reference group", fig.asp=0.4}
library(ggplot2)
# Create coordinates for the variable names.
variables <- data.frame(x = c(rep.int(0.3, times = 5), 0.7), 
                        y = c(.5, .4, .3, .2, .1, .3),
                        label = c("Exposure", "Former smoker", "Smoker", "Expo*Former", "Expo*Smoker", "Attitude"))
# Add coordinates for arrow endpoint.
x_diff <- 0.04
variables$xend <- variables$x[6] - x_diff #fixed translation to the left
variables$yend <- variables$y[6] + x_diff * (variables$y - variables$y[6]) / (variables$x[6] - variables$x)
rm(x_diff)
ggplot(variables, aes(x, y)) + 
  geom_segment(aes(x = variables$x[1], y = variables$y[1], xend = variables$xend[1], yend = variables$yend[1]), arrow = arrow(length = unit(0.04, "npc"), type = "closed")) + 
  geom_segment(aes(x = variables$x[2], y = variables$y[2], xend = variables$xend[2], yend = variables$yend[2]), arrow = arrow(length = unit(0.04, "npc"), type = "closed")) + 
  geom_segment(aes(x = variables$x[3], y = variables$y[3], xend = variables$xend[3], yend = variables$yend[3]), arrow = arrow(length = unit(0.04, "npc"), type = "closed")) + 
  geom_segment(aes(x = variables$x[4], y = variables$y[4], xend = variables$xend[4], yend = variables$yend[4]), arrow = arrow(length = unit(0.04, "npc"), type = "closed")) + 
  geom_segment(aes(x = variables$x[5], y = variables$y[5], xend = variables$xend[5], yend = variables$yend[5]), arrow = arrow(length = unit(0.04, "npc"), type = "closed")) + 
  geom_label(aes(label=label)) + 
  coord_cartesian(xlim = c(0.2, 0.8), ylim = c(0.1, 0.5)) +
  theme_void()
```

In the end, we have an interaction variable for all groups but one on the categorical moderator. Figure \@ref(fig:categorical-moderator-fig) shows the statistical diagram. Estimation of the model yields point estimates (regression coefficients), confidence intervals, and p values for all variables (Table \@ref(tab:cat-moderator-results)). 


```{r cat-moderator-results, echo=FALSE, message=FALSE, warning=FALSE}
# Table of regression coefficients for exposure moderated by status3. Similar to SPSS output (without correct standardized coefficients).
# Create effect sizes.
smokers <- haven::read_spss("data/smokers.sav")
model_1 <- lm(attitude ~ exposure*factor(status3, levels = c(0,1,2)), data = smokers)
# Table with results in SPSS style.
results <- coef(summary(model_1))
# Adjust parameter names
attributes(results)$dimnames[[1]][1] <- "(Constant)"
attributes(results)$dimnames[[1]][2] <- "Exposure"
attributes(results)$dimnames[[1]][3] <- "Former smoker"
attributes(results)$dimnames[[1]][4] <- "Smoker"
attributes(results)$dimnames[[1]][5] <- "Exposure*Former smoker"
attributes(results)$dimnames[[1]][6] <- "Exposure*Smoker"
# Confidence intervals
ci <- confint.lm(model_1)
results <- cbind(results, ci)
# Set column names.
attributes(results)$dimnames[[2]] <- c("B", "Std. Error", "t", "Sig.", "Lower Bound", "Upper Bound")
# Table.
options(knitr.kable.NA = '')
knitr::kable(results, digits = 3, caption = "Predicting attitude towards smoking for three smoking status groups: regression analysis results.")
# Cleanup.
rm(smokers, model_1, smokers, ci, results)
```

Remember that the effects of predictors that are included in interactions are conditional effects: effects for the reference group or reference value on the other variable involved in the interaction. The p value for Exposure tests the hypothesis that the exposure effect for people who never smoked is zero in the population. For the two dummy variables Former smoker and Smoker, the null hypothesis is tested that they have the same average attitude in the population as the non-smokers (reference group) if they are not exposed to the anti-smoking campaign (zero exposure).

Interaction predictors show effect differences. In Table \@ref(tab:cat-moderator-results), the interaction predictors test the null hypotheses that the effect of exposure is equal for former smokers and non-smokers (Exposure\*Former smoker) or for smokers and non-smokers (Exposure\*Smoker) in the population.

If we would like to know whether the exposure effect for former smokers is significantly different from zero, we have to rerun the regression model using former smokers as reference group. This new model would also tell us whether the exposure effect for people who stopped smoking is significantly different from the exposure effect for people who are still smoking.

### Common support {#commonsupportdichotomous}

In a regression model with moderation, we have to interpret the effect of a predictor involved in the interaction at a particular value of the moderator (Section \@ref(conditional-effects)). The estimated effect at a particular value of the moderator can only be trusted if there are quite some observations at or near this value of the moderator. In addition, these observations should cover the full range of values on the predictor. After all, the effect that we estimate must tell us whether higher values on the predictor go together with higher (or lower) values on the outcome.

For example, we need quite some observations for smokers to estimate the conditional effect of exposure on attitude for smokers. If there are hardly any smokers in our sample, we cannot estimate the effect of exposure on attitude for them in a reliable way. Even if we have quite some observations for smokers but all smokers have low exposure, we cannot say much about the effect of exposure on attitude for them. If we cannot say much about the effect within this group, we cannot say much about the difference between this effect and effects for other groups. In short, the moderation model is problematic here.

```{r common-support, fig.cap="How well do the observations cover the predictor within each category of smoking status?", echo=FALSE, screenshot.opts = list(delay = 5), dev="png", out.width="420px"}
# Goal: Sensitize students to the problem of lacking support for conditional
# effects by inspecting the coverage of the predictor for each moderator group.
# Generate a sample for smokers, for former smokers, and for non-smokers each of
# size 30. Give one or two randomly selected groups exposure values in the
# entire range [0, 10], but the remaining group(s) a restricted exposure range
# of 4 to 6 or 1 to 3 score points.
# Randomly assign a neutral, slightly negative, or moderately negative effect of
# exposure on attitude to the group. Display the three groups in a scatterplot
# (attitude by exposure) with different dot colours and their regression lines
# (coloured and labeled).
# Directly below the scatterplot, add a histogram of exposure, showing coverage.
# Allow the user to select groups 'All' (initial value), 'Smokers', 'Former
# smokers', or 'Non-smokers'. On selection, display the appropriate regression
# line and observations in the scatterplot and show their exposure scores in the
# histogram. Note that the scale of the histogram and the x axisof the
# scatterplot must be fixed to [0, 10].
# Add a Generate New button to generate a new dataset.
knitr::include_app("http://82.196.4.233:3838/apps/common-support/", height="580px")
```

1. What does the histogram represent in Figure \@ref(fig:common-support)?

```{r eval=FALSE}
* The histogram shows the counts of cases with particular exposure values.
* The counts are subdivided (coloured) by their score on the moderator variable
(smoking status).
```

2. Are the exposure values nicely spread for each smoke status group? Inspect each smoke status group separately with the "Show groups" option. 

```{r eval=FALSE}
* The answer depends on the sample that was drawn.
* For one moderator (smoking status) group, the predictor (exposure) may have
scores for the entire range, that is, from 0 to 10. But scores may also be
available only for part of this range, for example, from 1 to 6 or from 4 to
8.
```

3. What is the problem if exposure scores are not nicely spread over the same range for all smoke status groups?

```{r eval=FALSE}
* We may erroneously believe that the moderation effect applies to the entire
range of scores. For example, we may conclude that there is a negative effect
for smokers versus a positive effect for non-smokers. However, the effect for
smokers may be based on a range of exposure values that is different and
perhaps hardly overlapping with the range of exposure values for non-smokers.
In this case, the effect difference may be due to the level of predictor scores
instead of the moderator.
* In extreme cases, we may have only very few observations for a moderator
group, for example, only a handful of smokers. In this situation, the
regression effect for this group is not to be trusted.
```

The variation of predictor scores for a particular value of the moderator is called _common support_ [@RefWorks:3838]. If common support for predictors involved in moderation is bad, we should hesitate to draw conclusions from the estimated effects. Guidelines for good common support are hard to give. Common support is usually acceptable if there are observations over the entire range of the predictor. 

It is recommended to check the number of observations per value of the moderator. For a categorical moderator, such as smoking status, a scatter plot of outcome (vertical axis) by predictor (horizontal axis) with dots coloured according to the moderator category may do the job. Check that there are observations for more or less all values of the predictor. In Figure \@ref(fig:scatter-moderated), observations in each moderator category (dot colour) range from low to high predictor values.

```{r scatter-moderated, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="The effects of exposure to the anti-smoking campaign on attitude towards smoking among smokers and non-smokers."}
# Read data,
smokers <- haven::read_spss("data/smokers.sav")
# Graph of regression lines calculated in chunk dich-moderator-output.
# Plot.
ggplot(smokers, aes(x = exposure, y = attitude)) +
  geom_point(aes(colour = factor(status3, levels = c(0,1,2), 
                                 labels = c("Non-smoker", "Former smoker", "Smoker")))) +
  scale_x_continuous(limits = c(0, 10), breaks = c(0, 10),
                     labels = c("No exposure", "Maximum\nexposure")) +
  scale_y_continuous(limits = c(-5, 5), breaks = c(-5, 0, 5), 
                     labels = c("negative", "neutral", "positive")) +
  scale_colour_manual(values = c(unname(brewercolors["Blue"]), 
                                 unname(brewercolors["Orange"]), 
                                 unname(brewercolors["Red"]))) + 
  labs(x = "Exposure to the anti-smoking campaign", y = "Attitude towards smoking") +
  #Legend definitions
  guides(colour = guide_legend(title = "Smoking status:"),
             fill = FALSE) + 
  theme_general() + 
  theme(legend.position = "top")
# Cleanup.
rm(smokers)
```

```{r moderation-inference, fig.cap="Which null hypotheses are tested here?", eval=FALSE, echo=FALSE}
#DROPPED (added to Interpretation Section)

### Statistical inference for the interaction effect

# Goal: Understanding the null hypothesis for a conditional effect and an interaction effect by manipulating the population value of the conditional effects for (both) moderator groups.
# Generate a sample (N = 40?) from a population with exposure effect b = -.20 for non-smokers and b = .02 for smokers and an error term such that the former is significantly different from 0 but not the latter. Select a constant such that the predicted values are well within the [-5, 5] interval. Display a scatterplot (attitude by exposure) with the dots and regression lines for both groups. Add a table with the estimated regression coefficients for exposure, smoker, exposure-smoker interaction, and their standard error, t value, p value, and 95% confidence interval. Allow the user to change the conditional effects of exposure for both groups in the range [-.5, .5]. Then generate and display a new sample from a population with these values.

1. In Figure \@ref(fig:moderation-inference), what is the null hypothesis tested for the effect of exposure? 

2. How should you change the true values of the exposure effects to obtain a result that is not statistically significant?

3. What is the null hypothesis tested for the interaction effect of exposure with smoking status? 

4. How should you change the true values of the exposure effects to obtain a result that is not statistically significant?
```

## A Dichotomous or Categorical Moderator in SPSS {#catmodSPSS}

### Instructions

```{r SPSSregpred, echo=FALSE, out.width="640px", fig.cap="(ref:regpredSPSS)", dev="png", screenshot.opts = list(delay = 5)}
knitr::include_url("https://www.youtube.com/embed/pv6JnWL4nX8", height = "360px")
# Goal: Creating categorical by continuous interaction predictors (2 methods).
# Example: smokers.sav, predict the attitude towards smoking from exposure moderated by smoking status (variable status2 and status3), use contact with smokers as a covariate.
# SPSS menu: Transform > Compute Variable for status2 (already 0/1 variable) ; Transform > Create Dummy Variables for status3 (3 categories)
# Interpret output: none.

# Create interaction predictors (also for cont*cont interaction variables in addition to interactions with dummies) and dummies for main effects in one go: 
#   - ensure that categorical variables are marked as Nominal or Ordinal in Variable View
#   - command Transform > Create Dummy Variables ; select (numeric) predictor and (categorical) moderator under 'Create Dummy Variables for:'
#   - under Create main-effect dummies (option checked by default) specify a short name for both variables, separated with a comma ; the name of the numeric variable is irrelevant but must be specified
#   - ensure that the option _Do not create dummies for scale variables values_ is selected under Measurement Level Usage
#   - select the _Create dummies for all two-way interactions_ option under Two-Way Interactions and give a short name, e.g., interact
#   - Note: this procedure can also be used to create a numeric by numeric interaction variable
```

```{r SPSSregcatmod, echo=FALSE, out.width="640px", fig.cap="(ref:regcatmodSPSS)", dev="png", screenshot.opts = list(delay = 5)}
knitr::include_url("https://www.youtube.com/embed/A2VHIsYA66M", height = "360px")
# Goal: Estimating categorical by continuous moderation with regression in SPSS.
# Example: smokers.sav, predict the attitude towards smoking from exposure moderated by smoking status (variable _status2_), use contact with smokers as a covariate.
# SPSS menu: linear regression, include descriptives for means and standard deviations of covariates for making reference lines.
# Interpret output: use unstandardized regression coefficients because they show the average difference in slope (effect size) ; do not use the standardized regression coefficients that SPSS reports: they are calculated in the wrong way if the regression model includes an interaction variable. As a result, they are meaningless ; better interpret the regression lines in a scatterplot, see another video
# Check assumptions: See other video.
``` 

```{r SPSSregmodlines, echo=FALSE, out.width="640px", fig.cap="(ref:regmodlinesSPSS)", dev="png", screenshot.opts = list(delay = 5)}
knitr::include_url("https://www.youtube.com/embed/qZbdMdNoldI", height = "360px")
# Goal: Representing moderation by regression lines in a scatterplot in SPSS.
# Example: smokers.sav, predict the attitude towards smoking from exposure moderated by smoking status (variable _status2_), use contact with smokers as a covariate.
# Technique: linear regression
# SPSS menu: 
# Interpret output: 

# * Visualize categorical moderator with reference lines in scatterplot:
#   - {skip} No covariates: Graphs > Regression Variable Plots: outcome on Y, predictor on X, categorical moderator in Color by, Options>Scatterplot Fit Lines > Linear and Grouping > Fit line for each categorical colour group
#   - With covariates: 
#     - create scatterplot with Graphs > Legacy Dialogs > Scatter/Dot > Simple Scatter
#     - select outcome variable under Y Axis: and (numeric) predictor under X Axis: 
#     - select (original) categorical moderator under Set Markers by: (colours the observations according to moderator value, useful for inspecting common support)
#     - Paste & Run 
#     - display the mean values of covariates with Analyze > Descriptive Statistics > Fequencies ; select Statistics > Mean
#     - in SPSS Output, double-click the scatterplot to open it in the Chart Editor 
#     - add reference line (Options > Reference Line from Equation) ; in the Properties window under the Reference Line tab, add the regression equation for the first group in the moderator variable: use x for the predictor displayed on the X axis and use the category value (0/1) for dummies and the average values for numeric covariates
#     - repeat for other categories of the moderator variable
#     - change type (or colour) of the line in the Properties window under the Lines tab
#     - if you like, add label to lines describing the moderator group: Options > Text Box
#     - close the Chart Editor
```

```{r SPSSregSupport1, echo=FALSE, out.width="640px", fig.cap="(ref:regSupport1SPSS)", dev="png", screenshot.opts = list(delay = 5)}
knitr::include_url("https://www.youtube.com/embed/d93Kvm0imio", height = "360px")
# Goal: Check common support for a predictor at different moderator values in SPSS.
# Example: smokers.sav, predict the attitude towards smoking from exposure moderated by smoking status (variable _status2_), use contact with smokers as a covariate.
# Technique: 
# SPSS menu: 
# Interpret output: 
# Check assumptions: 
#
# * Check common support: 
#   - make histograms of the predictor panelled by the categorical grouping variable ; check that there are observations for more or less all values of the predictor (on the X axis) 
```

### Exercises

1. Use the data in [smokers.sav](http://82.196.4.233:3838/data/smokers.sav) to predict the attitude towards smoking from exposure moderated by smoking status (variable _status2_). Use contact with smokers as a covariate. Check the assumptions for regression analysis and interpret the results.

```{r eval=FALSE}
SPSS syntax:  
  
* Check data.  
FREQUENCIES VARIABLES=exposure status2 contact attitude  
  /ORDER=ANALYSIS.  
* Compute interaction variable.  
COMPUTE expo_status=exposure * status2.  
VARIABLE LABELS  expo_status 'Interaction exposure * smoker'.  
EXECUTE.  
* Multiple regression.  
REGRESSION  
  /MISSING LISTWISE  
  /STATISTICS COEFF OUTS CI(95) R ANOVA  
  /CRITERIA=PIN(.05) POUT(.10)  
  /NOORIGIN   
  /DEPENDENT attitude  
  /METHOD=ENTER exposure status2 expo_status contact  
  /SCATTERPLOT=(*ZRESID ,*ZPRED)  
  /RESIDUALS HISTOGRAM(ZRESID).  
  
Check data:  
  
All values seem to be valid.  
  
Check assumptions:  
  
* The residuals are skewed (long tail to the left).
* The residuals seem to average to zero on most levels of the predicted
outcome, so a linear model seems to fit.
* However, the lower attitude values are predicted worse (more variation) than
the higher levels. The assumptions do not seem to be strongly violated but our
model may not be specified all well.
  
Interpret the results:  
  
Add table with regression coefficients.  
  
* We can predict smoking attitude for about 26 per cent with the regression
model, R2 = .26, F (4, 80) = 7.10, p < .001.
* The predictive effect of exposure to the anti-smoking campaign on smoking
attitude for non-smokers is more probably negative than positive but it is not
significantly different from zero, b = -0.12, t = -1.79, p = .078,
95%CI[-0.25, 0,01]). More exposure tends to yield a more negative attitude.
* For smokers, the predictive effect of campaign exposure on smoking attitude
is more strongly negative. The moderation of the exposure effect by smoking
status is negative and statistically significant, b = -0.33, t = -2.35, p =
.021, 95%CI[-0.61, -0.05].
* More contact with smokers is associated with a more positive attitude
towards smoking rather than a negative attitude. The predictive effect is weak
(b* = 0.18) but not significantly different from zero, b = 0.15, t = 1.68, p =
.096, 95%CI[-0.03, 0.33].
* Smokers have a more positive attitude than non-smokers if they are not
exposed to the campaign, on average circa 2 (0.5 to 3.4) points more positive
the attitude, and this difference is statistically significant, b = 1.98, t =
2.72, p = .008, 95%CI[0.53, 3.43].
  
Remember:   
* In the presence of an interaction effect, the partial effect of a predictor
is the effect for the reference group or value. It is NOT an overall or
average effect as in analysis of variance.
* Standardized regression coefficients reported by SPSS are not correct for
interaction effects or effects of predictors that are involved in interaction
effects. They can only be used for predictors that are not involved in
interaction effects.
* Contact with smokers is not involved in an interaction in this model, so we
can interpret the standardized regression coefficient for this effect.
```

2. Visualize the moderated effects of exposure on attitude (Exercise 1). Create a scatter plot with two regression lines. Colour the regression lines and the dots (respondents) according to their smoking status category. Interpret how smoking status moderates the effect of exposure on attitude.

```{r eval=FALSE}
* First of all, you must write the regression equations for different values
of the moderator. Plug in the estimated values of the regression coefficients
and the means of covariates (here: average contact with smokers).

* SPSS syntax to get the average value of contact with smokers:
FREQUENCIES VARIABLES=contact  
 /FORMAT=NOTABLE  
 /STATISTICS=MEAN  
 /ORDER=ANALYSIS.  
  
attitude = constant + -.118*exposure + 1.982*status + -.329*exposure*status + .152*contact  
  
attitude = -.087 + -.118*exposure + 1.982*status + -.329*exposure*status + .152*5.091  
  
attitude = -.087 + -.118*exposure + 1.982*status + -.329*exposure*status +.774  
  
attitude = .687 + -.118*exposure + 1.982*status + -.329*exposure*status  

* Replace status by 0 for non-smokers.

Non-smokers (0):  
  
attitude = .687 + -.118*exposure + 1.982*0 + -.329*exposure*0  
  
attitude = .687 + -.118*exposure  
  
* Replace status by 1 for smokers.

Smokers (1):  
  
attitude = .687 + -.118*exposure + 1.982*1 + -.329*exposure*1  
  
attitude = .687 + 1.982 + -.118*exposure + -.329*exposure  
  
attitude = 2.669 + (-.118 + -.329)*exposure  
  
attitude = 2.669 + -.447*exposure  
  
* Next, create a scatterplot of attitude by exposure, colouring the dots by
smoking status. Use the calculated two equations in the SPSS Chart Editor to
create two lines. Use the icon "Add a reference line from Equation" for each
line. Enter the equation using x instead of exposure as the predictor.
Colour the lines with the colours of the dots in the scatterplot.
  
SPSS syntax:  
  
* Scatterplot with dots coloured by smoking status.  
GRAPH  
  /SCATTERPLOT(BIVAR)=exposure WITH attitude BY status2  
  /MISSING=LISTWISE.  
  
Check data:  
  
See Exercise 1.  
  
Check assumptions:  
  
See Exercise 1.  
  
Interpret the results:  
  
* For smokers, the predictive effect of campaign exposure on smoking attitude
is more strongly negative (b = -0.45) than for non-smokers (b = -0.12.
```

3. Check common support of the predictor (exposure) in all groups of the moderator (smoking status). Could you also check common support with the scatter plot you made for Exercise 2?

```{r eval=FALSE}
SPSS syntax:  
  
* Histogram of predictor (exposure) for each smoking status.  
GRAPH  
  /HISTOGRAM=exposure  
  /PANEL ROWVAR=status2 ROWOP=CROSS.  
  
Interpret the results:  
  
* Even for smokers, the much smaller group, we have exposure scores over
(almost) the entire range. We have good coverage both for smokers and
non-smokers. Do not mind the gap in scores around 6: we have plenty of
observations around 5 and 7.
  
* We could have seen this result in the scatterplot because we had green and
blue dots across the entire width of the plot.
```

4. Repeat the analyses of Exercises 1 through 3 but use smoking status with three categories (_status3_).

```{r eval=FALSE}
SPSS syntax:  
  
* Check data.  
FREQUENCIES VARIABLES=exposure status3 contact attitude  
  /ORDER=ANALYSIS.  
* Create dummies and interaction variables.  

* With Create Dummy Variables.  
* ENSURE THAT MEASUREMENT LEVEL IS SET TO ORDINAL.   
* Define Variable Properties.  
*status3.  
VARIABLE LEVEL  status3(ORDINAL).  
EXECUTE.  
SPSSINC CREATE DUMMIES VARIABLE=exposure status3   
ROOTNAME1=exposure, status ROOTNAME2=expo_status   
/OPTIONS ORDER=A USEVALUELABELS=YES USEML=YES OMITFIRST=NO.  

* With Recode.  
RECODE status3 (1=1) (ELSE=0) INTO status_3.  
VARIABLE LABELS  status_3 'Former smoker'.  
EXECUTE.  
RECODE status3 (2=1) (ELSE=0) INTO status_4.  
VARIABLE LABELS  status_4 'Smoker'.  
EXECUTE.  
* Interaction variables (same name as those given by Create Dummy Variables).  
COMPUTE expo_status_2_2=exposure * status_3.  
VARIABLE LABELS  expo_status_2_2 'expo * formersmoker'.  
EXECUTE.  
COMPUTE expo_status_2_3=exposure * status_4.  
VARIABLE LABELS  expo_status_2_3 'expo * smoker'.  
EXECUTE.  

* Multiple regression.  
* Statistic Descriptives is added to get the means that we need  
* to plug into the regression equation in the moderation plot.  
REGRESSION  
  /DESCRIPTIVES MEAN STDDEV CORR SIG N  
  /MISSING LISTWISE  
  /STATISTICS COEFF OUTS CI(95) R ANOVA  
  /CRITERIA=PIN(.05) POUT(.10)  
  /NOORIGIN   
  /DEPENDENT attitude  
  /METHOD=ENTER exposure status_3 status_4 expo_status_2_2 expo_status_2_3 contact  
  /SCATTERPLOT=(*ZRESID ,*ZPRED)  
  /RESIDUALS HISTOGRAM(ZRESID).  
* Scatterplot with dots coloured by smoking status.  
GRAPH  
  /SCATTERPLOT(BIVAR)=exposure WITH attitude BY status3  
  /MISSING=LISTWISE.  
* Histogram of predictor (exposure) for each smoking status.  
GRAPH  
  /HISTOGRAM=exposure  
  /PANEL ROWVAR=status3 ROWOP=CROSS.  
  
* Write out the regression equations for all three groups of the moderator.
Plug in the estimated values of the constant and the regression coefficients.
  
attitude = .550 + -.137*exposure + -1.139*former + 1.223*smoker + 
  -.402*exposure*former + -.304*exposure*smoker + .171*contact  
  
Plug in the means of covariates and add to constant:  
    
attitude = .550 + -.137*exposure + -1.139*former + 1.223*smoker + 
  -.402*exposure*former + -.304*exposure*smoker + .171*5.091  
  
attitude = 1.421 + -.137*exposure + -1.139*former + 1.223*smoker + 
  -.402*exposure*former + -.304*exposure*smoker  
  
Non-smokers (former = 0, smoker = 0):  
  
attitude = 1.421 + -.137*exposure + -1.139*0 + 1.223*0 + -.402*exposure*0 + 
  -.304*exposure*0  
  
attitude = 1.421 + -.137*exposure  
  
Former smokers (former = 1, smoker = 0):  
  
attitude = 1.421 + -.137*exposure + -1.139*1 + 1.223*0 + -.402*exposure*1 + 
  -.304*exposure*0  
  
attitude = 1.421 + -1.139 + -.137*exposure + -.402*exposure  
  
attitude = 0.282 + (-.137 + -.402)*exposure  
  
attitude = 0.282 + -.539*exposure  
  
Smokers (former = 0, smoker = 1):  
  
attitude = 1.421 + -.137*exposure + -1.139*0 + 1.223*1 + -.402*exposure*0 + 
  -.304*exposure*1  
  
attitude = 1.421 + 1.223 + (-.137 + -.304)*exposure  
  
attitude = 2.644 + -.441*exposure  
  
* Create a scatterplot of attitude by exposure, colouring the dots by smoking
status.
* Use the calculated two equations in the SPSS Chart Editor to create two
lines. Use the icon "Add a reference line from Equation" for each line. Enter
the the equation using x instead of exposure as the predictor. Colour the
lines with the colours of the dots in the scatterplot.
  
Check data:  
  
All values seem to be valid.  
  
Check assumptions:  
  
* The residuals seem to be skewed a little bit.  
* The residuals by predicted values plot gives no reason to doubt the
linearity of the model but the problem of predicting higher values less
accurately than lower values seems to be worse than in Exercise 1. We should
warn the reader that the assumptions seem to be violated.
  
Interpret the results:  
  
* Again, summarize the tests on the regression coefficients in a table.  
  
* With three smoking status groups, we can predict attitude towards smoking
much better (R2 = 0.70) than with the two groups in Exercise 1 (R2 = .26).
  
* There is an important difference between non-smokers and former smokers
(they were lumped together in the preceding exercises). On average, former
smokers have a more negative attitude than non-smokers, which may range from a
tiny difference (-0.1) to a large difference (-2 points on the scale from -5 to
+5) if they are not exposed to the campaign.
  
* In addition, exposure has a stronger negative predictive effect on smoking
attitude among former smokers than among non-smokers. Exposure also has a
stronger negative effect on attitude among smokers than among non-smokers. For
short, exposure to the campaign has less impact on attitude towards smoking
for non-smokers than for former smokers or smokers.
  
* The coverage of exposure is good for non-smokers and smokers but former
smokers with high exposure are rare.
```

## A Continuous Moderator {#cont-moderator-regression}

With a categorical moderator, it is quite obvious for which values of the moderator we are going to calculate and depict the effect of the predictor on the outcome. If smoking status moderates the effect of exposure on attitude towards smoking, we will inspect a regression line for each smoking status category: smokers, former smokers, and non-smokers. But what if the moderator is a continuous, numeric variable, for example, the intensity of contact with smokers?

```{r continuous-moderator, fig.cap="How do contact values affect the conditional effect of exposure on attitude?", echo=FALSE, screenshot.opts = list(delay = 5), dev="png", out.width="440px"}
# Goal: Understand that there is a conditional effect for each value of the moderator by gradually changing the moderator value & understanding the linearity of the effect: a fixed slope change for a fixed difference in moderator values.
# Generate a data set with a linear interaction (attitude ~ exposure*contact). Display a scattergram with a regression line for the current value of the moderator (contact). Display regression equation as y = a + (b_1 + b_3*contact(5))*exposure + b_2*contact(5) with values for coefficients and for contact. Add slider allowing the user to change the moderator value (range [0, 10], initial value 0). Replace the previous regression line by a grey line, remove older regression lines, and add new regression line in black; also update regression equation.
# # Number of observations.
# n <- 85
# # Create predictor.
# set.seed(4932)
# exposure <- runif(n)*10
# # Create moderator.
# set.seed(4321)
# contact <- 0.12*(10 - exposure) + rnorm(n, mean = 4.5, sd = 2)
# # Create outcome.
# set.seed(390)
# attitude <- -0.26*exposure + 0.15*contact + 0.04*exposure*contact + rnorm(n, mean = 2, sd = 0.5)
knitr::include_app("http://82.196.4.233:3838/apps/continuous-moderator/", height="550px")
```

1. The regression line depicted in Figure \@ref(fig:continuous-moderator) represents the conditional effect of exposure on attitude for the value of contact with smokers selected with the slider. How many different conditional effects are there?

```{r eval=FALSE}
* In principle, there is an unlimited number of conditional effects if the
moderator is a continuous variable.
* In the app, however, the slider allows you to increase or decrease contact
score by 0.5. In the app, there are effectively 21 moderator values that you
can select, so there are 21 different regression lines that can be depicted.
```

2. Is the effect of campaign exposure on attitude towards smoking always negative? Or does more exposure lead to a more positive attitude (higher score) in some cases? If so, in which cases?
```{r eval=FALSE}
* Move the slider from left to right to find the moderator value at which the
regression line is horizontal. For higher moderator values, the slope of the
regression line is positive. Here, more exposure leads to a more positive
attitude.
* Or have a close look at the equations. The regression coefficient of the
(simple) slope of the exposure effect is zero if the part between brackets (b1
+ b3*contact) is zero. We know b1 and b3, so we must solve the equation:

> -0.26 + 0.04 * contact = 0

Your high-school algebra may help you:

> 0.04 * contact = 0.26

> contact = 0.26 / 0.04

> contact = 0.26 / 0.04 = 6.5
```

3. How much does the slope increase if the moderator value is changed from 0 to 1? And how much if it changes from 6 to 7? 
```{r eval=FALSE}
* Each increment of 1 unit of contact increases the (simple) slope of the
exposure effect on attitude by 0.04, that is, by the value of the interaction
effect.
* This is easy to see in the equation for the effect of exposure:

> (-0.26 + 0.04 * contact) * exposure

* Plug in 0 for contact: The simple slope is -0.26.
* Plug in 1 for contact: The simple slope is -0.26 + 0.04 * 1.
* The difference is 0.04. This difference is the same for every increase of
one unit in contact, so it is also the difference between the slopes at
contact levels six and seven.
```

People hanging around a lot with smokers are likely to have a more positive attitude towards smokers than people who have little contact with smokers. After all, people who really hate smoking will avoid meeting smokers. This is a main effect of contact with smokers on attitude towards smoking.

In addition, the anti-smoking campaign may be less effective for people who spend a lot of time with smokers. Negative perceptions of smoking instilled by the campaign can be compensated by positive experiences of seeing people enjoy smoking. Contact with smokers would decrease the effect of campaign exposure on attitude. The effect of exposure is moderated by contact with smokers.

Our moderator, contact with smokers, is continuous. As a consequence, we can have an endless number of contact levels as groups for which the slope may change. This is the only difference with a categorical moderator. Other than that, we will analyze a continuous moderator in the same way as we analyzed a categorical moderator.

### Interaction variable {#interpret-cont-interaction}

We need one interaction variable to include a continuous moderator in a regression model. As before, the interaction variable is the product of the predictor and the moderator. Multiply the predictor with the moderator to obtain the interaction variable.

Although we have an endless number of different moderator values or groups, we only need one interaction variable. It represents the gradual (linear) change of the effect of the predictor for higher values of the moderator. 

\begin{equation}
\small
\begin{split}
  attitude = &\ constant + b_1*exposure + b_2*contact + b_3*exposure*contact + e \\
  attitude = &\ constant + (b_1 + b_3*contact)*exposure + b_2*contact + e 
\end{split}
(\#eq:simplecontact) 
\normalsize
\end{equation}

To see this, it is helpful to inspect the regression equation with rearranged terms (Equation \@ref(eq:simplecontact)). Every extra contact with smokers adds $b_3$ to the slope $(b_1 + b_3*contact)$ of the exposure effect. The addition is gradual---a little bit of additional contact with smokers changes the exposure effect a little bit---and it is linear: A unit increase in contact adds the same amount to the effect whether the effect is at a low or a high level.

We can interpret the regression coefficient of the interaction effect ($b_3$) here as the predicted change in the exposure effect (slope) for a one unit difference in contact (the moderator). A positive coefficient indicates that the exposure effect is more positive for higher levels of contact with smokers. A negative coefficient indicates that the effect is more negative for people with more contacts with smokers. 

Note that positive and negative are used here in there mathematical meaning, not in an appreciative way. A positive effect of exposure implies a more positive attitude towards smoking. Anti-smoking campaigners probably evaluate this as a negative result.

### Conditional effect {#conditional-effect-cont}

The regression coefficients for exposure and contact represent conditional effects (see Section \@ref(conditional-effects)), namely, the effects for cases that score zero on the other variable. Plug in zero for the moderator and you will see that all terms with a moderator drop from the equation and only $b_1$ is left as the effect of exposure.  

\begin{equation}
\small
\begin{split}
  attitude = &\ constant + (b_1 + b_3*contact)*exposure + b_2*contact + e\\
  attitude = &\ constant + (b_1 + b_3*0)*exposure + b_2*0 + e\\
  attitude = &\ constant + b_1*exposure + e 
\end{split}
(\#eq:conditionaleffect) 
\normalsize
\end{equation}

The zero score on the moderator is the _reference value_ for the conditional effect of the predictor. Cases that score zero on the moderator are the _reference group_ just like cases scoring zero on the dummy variables are the reference group in a model with a categorical moderator (Section \@ref(dichpredictor)). 

### Mean-centering

Because the effects of predictors involved in an interaction are conditional effects, a zero score on these variables has a special role. It is the reference value for the effect of the other predictor. For example, the effect of exposure on attitude applies to respondents with zero contacts with smokers if the regression model includes an exposure by contact interaction. If zero is so important, we may want to manipulate this value.

```{r mean-centering-moderator, eval=TRUE, echo=FALSE, fig.cap="What happens if you mean-center the moderator variable?", echo=FALSE, out.width="420px", screenshot.opts = list(delay = 5), dev="png"}
# Goal: Understand how mean-centering affects the interpretion of the
# conditional effect of the predictor by seeing how the reference value changes
# with mean-centering (and centering on another value, e.g., M plus/minus 1 SD).
# Use same data set as in app continuous-moderator. 
# Display a scatterplot with a line representing the conditional regression line for exposure at moderator (contact) value = 0. Shade dots in scatterplot in accordance with distance of their moderator score to the moderator reference value. 
# Display the current reference value of the moderator: "Line for contact = 0"
# Add a slider 'Contact - x' (equal length as x axis, range [0, 10], initial value 0), labeled with values 0, M - SD, M, M + SD, and 10 (use nice round numbers for SD and M, not exactly the true values). Adjusting the slider updates the regression line in the scatterplot and the reference value label.
knitr::include_app("http://82.196.4.233:3838/apps/mean-centering-moderator/", height="510px")
```

1. What is the correct interpretation of the estimated regression coefficient of exposure in Figure \@ref(fig:mean-centering-moderator)?

```{r eval=FALSE}
* In the initial situation, the regression line in this figure represents the
predictive effect of exposure on attitude for respondents who score zero on the
moderator (Contact with smokers).
* In this regression model, the regression coefficient of exposure is -0.26,
so an additional unit of exposure decreases the predicted attitude by 0.26 for
respondents who have no (zero) contacts with smokers.
```

2. What happens to the red regression line and the regression equation if you subtract the mean (*M*) from the respondents' contact with smokers scores? Thus, you mean-center the moderator Contact. Use the slider **Center Contact. Subtract from Contract:** in Figure \@ref(fig:mean-centering-moderator) to check your answer.

```{r eval=FALSE}
* Mean Contact score is 5. The regression coefficient of exposure changes to
-0.06, namely -0.26 + 0.04 * 5. Accordingly, the red regression line becomes
almost horizontal.
```

3. If Contact is mean-centered, the regression coefficient of exposure represents the effect of exposure on attitude for respondents with a particular score on the original Contact variable. What is this original Contact score? Use the slider **Adjust the value of Contact (Moderator):** to check your answer.

```{r eval=FALSE}
* The regression line of the mean-centered Contact moderator coincides with
the blue regression line if we select 5 as value of the original Contact
variable. 
* Apparently, the reference group for the regression coefficient of exposure in
case of a mean-centered moderator, consists of respondents who score 5 on the
original Contact with smokers moderator.
* By mean centering, we assign the score zero to respondents who used to have
a mean score on the moderator Contact with smokers. Now that they score zero
on the new mean-centered variable, they have become the reference group.
```

What if there are no people with zero contact? Then, the interpretation of the regression coefficient $b_1$ for exposure does not make sense. In this situation, it is better to mean-center the moderator (contact) before you add it to the regression equation and before you calculate the interaction variable. 

To _mean-center_ a variable, you subtract the variable's mean from all scores on the variable. As a result, a mean score on the original variable becomes a zero score on the mean-centered variable. 

\begin{equation*}
\small
  contactcentered = contact - mean(contact)
\normalsize
\end{equation*}

With mean-centered numerical moderators, a conditional effect in the presence of interaction always makes sense. It is the effect of the predictor for average score on the moderator. An average score always falls within the range of scores that actually occur. If we mean-center the contact with smokers moderator, the regression coefficient $b_1$ for exposure expresses the effect of exposure on attitude for people with average contacts with smokers. This makes sense.

Remember that the interaction variable is the product of the predictor and moderator (Section \@ref(interaction-variable)). If any or both of these are mean-centered, you should multiply the mean-centered variable(s) to create the interaction variable, see Sections \@ref(conditional-effects) and \@ref(conditional-effect-cont).

### Symmetry of predictor and moderator

```{r symmetry-predictor-moderator, eval=FALSE, echo=FALSE, fig.cap=""}
# Goal: Understand the advantages of mean-centering the predictor by seeing how
# the reference value changes with mean-centering (and centering on another
# value, e.g., M plus/minus 1 SD).
# Use same data set as in app continuous-moderator: predictor = exposure,
# moderator = contact.
# Display scatterplot (x axis not labelled) with conditional regression effect
# for predictor (blue) at moderator value = 0 and conditional effect of
# moderator (red) for predictor = 0.
# Show two additional x axes marking the reference values of the predictor
# (blue) and moderator (red) (range [0, 10], initial value 0).
# Add sliders 'Exposure - x' and 'Contact - x' (equal length as two x axes,
# range [0, 10], initial value 0), labeled with values M - SD, M, and M + SD.
# Adjusting the sliders update the scale of the appropriate x axis (the marked
# point zero moves) and the regression lines in the scatterplot.

1. If you change the value on the slider 'Exposure - x', which regression line in the plot changes? Why this line?

2. Which variable is the predictor and which is the moderator if you adjust the value of the slider 'Exposure - x'?
```

If we want to interpret the conditional effect of contact on attitude ($b_2$), we must realize that this is the effect for people who score zero on the exposure variable if the exposure by contact interaction is included in the regression model. This is clear if we rearrange the regression equation as in Equation \@ref(eq:contactbyexposure).

\begin{equation}
\small
\begin{split}
  attitude = &\ constant + b_1*exposure + b_2*contact + b_3*exposure*contact + e\\
  attitude = &\ constant + b_1*exposure + (b_2 + b_3*exposure)*contact + e\\
  attitude = &\ constant + b_1*0 + (b_2 + b_3*0)*contact + e\\
  attitude = &\ constant + b_2*contact + e\end{split}
(\#eq:contactbyexposure) 
\normalsize
\end{equation}

But wait a minute, this is what we would do if contact was the predictor and exposure the moderator. That is a completely different situation, is it not? No, technically it does not make a difference which variable is the predictor and which is the moderator. The predictor and moderator are symmetric. The difference is only in our theoretical expectations and in our interpretation.

The conditional effect of the moderator, as stated above, is the effect of the moderator if the predictor is zero. This interpretation makes sense only if there are cases with zero scores on the predictor. In the current example, the scores on exposure range from 0 to 10, so zero exposure is meaningful. But it represents a borderline score with perhaps a very atypical effect of contact on attitude or few observations. For these reasons, it is recommended to *mean-center both the predictor and moderator if they are numeric*.

### Visualization of the interaction effect

It can be quite tricky to interpret regression coefficients in a regression model that contains interaction effects. The safest strategy is to draw regression lines for different values of the moderator. But what are interesting values if the moderator is numerical?

```{r continuous-interaction-visualization, fig.cap="Which moderator values are helpful for visualizing moderation?", echo=FALSE, screenshot.opts = list(delay = 5), dev="png", out.width="440px"}
# Goal: Clarify the interpretation of the (unstandardized) interaction effect by
# showing regression lines at different (interesting) moderator scores (display
# slope value).
# Variant of the app continuous-moderator; ensure that there are few predictor
# values at the minimum and maximum values of the moderator. Allow user to pick
# several values for the moderator from a list containing: minimum, maximum,
# first tercile, second tercile, M - 2SD, M - 1SD, M, M + 1SD, M
# + 2SD. Display the selected lines in different colours.
knitr::include_app("http://82.196.4.233:3838/apps/continuous-interaction-visualization/", height="390px")
```

1. Select one or more options in Figure \@ref(fig:continuous-interaction-visualization) to represent regression lines predicting attitude from exposure at different values of the moderator (contact with smokers). Respondents with moderator values close to the selected value are coloured. Which moderator values would you pick to communicate the results of moderation? Motivate your answer.

```{r eval=FALSE}
* The important thing is to understand that a regression line that depends on
few observations is not very trustworthy. The line usually does not represent
the observations well. In addition, these few observations may have different
predictor and outcome values in a new sample, so the regression line may be
quite different at this moderator value in a new sample.
* The minimum and maximum value observed in the current sample represent
regression lines that are quite outside the dot cloud. They are based on very
few observations, so they are not very trustworthy.
* Observations with moderator values at two standard deviations away from the
mean (options: M - 2SD and M + 2SD), are still quite rare.
* Observations with moderator values at one standard deviations away from the
mean (options: M - 1SD and M + 1SD), are quite common. These regression
lines are nicely embedded in the dot cloud. They are good candidates for
interpreting moderation.
* Of course, moderator values even closer to the mean, such as moderator values
below or above which we find one third of all scores (the first and third
terciles) are also well supported by observations, as is the moderator mean
itself.
```

As we have seen in Section \@ref(interpret-cont-interaction), the regression coefficient of an interaction effect with a continuous moderator can be directly interpreted. It represents the predicted difference in the unstandardized effect size for a one unit increase in the moderator. For example, one more contact with a smoker increases the exposure effect by 0.04.

The size of the interaction effect tells us the moderation trend, for instance, people who are more around smokers tend to be less opposed to smoking if they are exposed to the anti-smoking campaign. But we do not know how much an anti-smoking attitude is fostered by exposure to a campaign and whether exposure to the campaign increases anti-smoking attitude for everyone. Perhaps, people hanging out with smokers a lot may even get a more positive attitude towards smoking from campaign exposure.

We can be more specific about exposure effects at different levels of contact with smokers if we pick some interesting values of the moderator and calculate the conditional effects at these levels.

The minimum or maximum values of the moderator are usually not very interesting. We tend to have few observations for these values, so our confidence in the estimated effect at that level is low. Instead, the values one standard deviation below and above the mean of the moderator are popular values to be picked. One standard deviation below the mean (M - SD) indicates a low value, the mean (M) indicates a central value, and one standard deviation above the mean (M + SD) indicates a high value. 

Having picked these values, we can visualize moderation as different regression lines in a plot. We use exactly the same approach as in visualizing moderation by a categorical variable. As a first step, we construct equations for conditional effects of the predictor at different levels of the moderator. Plug the selected value of the moderator into the regression equation. If there are covariates, also plug in a meaningful value for the covariates, usually the average for numeric covariates and zero or one for dichotomous covariates. As a second step, we use the equations to add regression lines to a scatter plot.

If contact (the moderator) is mean-centered, as in the current example, we simply plug in zero for the moderator to obtain the equation for the regression line at the mean of the moderator (contact with smokers). We plug in the value of the standard deviation of contact with smokers to get the regression equation for people who scored one standard deviation above the mean on the moderator. The standard deviation of contact is 2.0 in this example, so Equation \@ref(eq:regsimpleslopemoderated) replaces contact by 2.0 everywhere. Finally, we plug in minus the value of one standard deviation if we want the regression line for the moderator at the mean minus one standard deviation.

\begin{equation}
\small
\begin{split}
  attitude &= 3.6 + -0.1*exposure + 0.1*status + 0.1*contact \\
  &\ + 0.03*contact*exposure \\
  attitude &= 3.6 + -0.1*exposure + 0.1*(0) + 0.1*(2.0) + 0.03*(2.0)*exposure \\
    attitude &= 3.8 + -0.04*exposure 
\end{split}
(\#eq:regsimpleslopemoderated) 
\normalsize
\end{equation}

We also have to plug in a value for each covariate. This example contains one covariate, namely (smoking) status. We plug in the score for non-smokers (0). In the end, our predictor (exposure) should be the only variable in the right hand side of the regression equation (the last line in Equation \@ref(eq:regsimpleslopemoderated)). Note that we do not include the error term ($e$) in the equation if we predict values; the error term captures prediction errors.

If the moderator is not mean-centered, we have to plug in the value of the mean of the moderator and the value of the mean plus or minus the standard deviation of the moderator. In this example, the mean score of contact with smokers is 5.1, so the moderator mean minus one standard deviation (2.0) equals 3.1 and the mean plus one standard deviation is 7.1.

### Statistical inference on conditional effects

```{r cont-moderator-output, echo=FALSE, message=FALSE, warning=FALSE}
# Table of regression coefficients for the effect of exposure moderated by contact with smokers. Similar to SPSS output (with standardized coefficients?).
# Create effect sizes.
smokers <- haven::read_spss("data/smokers.sav")
# Mean-center numerical predictors.
smokers$exposure_mc <- smokers$exposure - mean(smokers$exposure)
smokers$contact_mc <- smokers$contact - mean(smokers$contact)
# Unsandardized linear model.
model_1 <- lm(attitude ~ exposure_mc*contact_mc + status2, data = smokers)
# Table with results in SPSS style.
results <- coef(summary(model_1))
# Adjust parameter names
attributes(results)$dimnames[[1]][1] <- "(Constant)"
attributes(results)$dimnames[[1]][2] <- "Exposure (mean-centered)"
attributes(results)$dimnames[[1]][3] <- "Contact (mean-centered)"
attributes(results)$dimnames[[1]][4] <- "Status (smoker)"
attributes(results)$dimnames[[1]][5] <- "Exposure*Contact (mean-centered)"
# Confidence intervals
ci <- confint.lm(model_1)
results <- cbind(results, ci)
# Correctly standardized coefficients.
attach(smokers)
z_exposure <- (exposure_mc - mean(exposure_mc)/sd(exposure_mc))
z_contact <- (contact_mc - mean(contact_mc)/sd(contact_mc))
z_status2 <- (status2 - mean(status2)/sd(status2))
z_expocontact <- (exposure_mc * contact_mc - mean(exposure_mc * contact_mc)/sd(exposure_mc * contact_mc))
z_attitude <- (attitude - mean(attitude)/sd(attitude))
model_2 <- lm(z_attitude ~ z_exposure + z_contact + z_status2 + z_expocontact)
results_2 <- coef(summary(model_2))
results <- cbind(results[, 1:2], results_2[, 1], results[, 3:6])
results[1, 3] <- NA
# Set column names.
attributes(results)$dimnames[[2]] <- c("B", "Std. Error", "Beta", "t", "Sig.", "Lower Bound", "Upper Bound")
# Table.
options(knitr.kable.NA = '')
knitr::kable(results, digits = 3, caption = "Predicting attitude towards smoking: regression analysis results with exposure and contact mean-centered.")
# Helper function for displaying results within the text.
source("report_n.R")
# Partial cleanup.
rm(smokers, model_1, smokers, ci, results_2, model_2, z_attitude, z_contact, z_status2, z_expocontact, z_exposure)
```

The regression model yields a p value and confidence interval for the predictor at the reference value of the moderator. In the model estimated in Table \@ref(tab:cont-moderator-output), for instance, we obtain a p value of `r report_n(results[2,5], 3)` and a 95% confidence interval of [`r report_n(results[2,6],2)`, `r report_n(results[2,7],2)`] for the effect of exposure on attitude. This is the conditional effect of exposure on attitude for cases that score zero on the moderator variable (contact with smokers) as we can verify in Equation \@ref(eq:statinfconditional).

\begin{equation}
\small
\begin{split}
  attitude = &\ constant + b_1*exposure + b_2*contact + b_3*status +\\
  &\ + b_4*exposure*contact + e\\
    attitude = &\ constant + (b_1 + b_3*contact)*exposure + b_2*contact + e\\
        attitude = &\ constant + (b_1 + b_4*0)*exposure + b_2*0 + b_3*status + e\\
        attitude = &\ constant + b_1*exposure + b_3*status + e
\end{split}
(\#eq:statinfconditional) 
\normalsize
\end{equation}

If the variable contact is mean-centered, the p value tests the null hypothesis that the effect of exposure is zero for people who have average contact with smokers. The confidence interval tells us that the effect of exposure on attitude for people with average contacts with smokers ranges between `r report_n(results[2,6],2)` and `r report_n(results[2,7],2)` with 95% confidence. If the moderator is not mean-centered, the results apply to people who have no contact with smokers.

Note that mean-centering of the moderator changes, so to speak, the regression line that we test from the effect of exposure for people with no smoker contact to the effect for people with average contact with smokers. If we would like to get the p value or confidence interval for the regression line at one standard deviation above or below the mean, we have to center the moderator at those values before we estimate the regression model.

```{r echo=FALSE}
#Cleanup.
rm(results, report_n)
```

### Common support

In Section \@ref(commonsupportdichotomous), we checked the support of the predictor in the data for different groups of the moderator. The basic idea is that we can only sensibly estimate and interpret a conditional effect at a moderator level if we have observations over the entire range of the predictor. For each moderator group, we checked the distribution of the predictor.

With a continuous moderator we can also do this if we group moderator scores. Hainmueller et al. [-@RefWorks:3838] recommend creating three groups, each containing one third of all observations. These low, medium, and high groups correspond more or less with the minus one standard deviation/mean/plus one standard deviation values that we used for visualizing and testing conditional effects. Create a histogram for the predictor in each of these groups to check common support of moderation in the data.

### Assumptions

The general assumptions for regression analysis (Section \@ref(regr-inference)) also apply to the interaction effect with a continuous moderator. The checks are the same: See if the residuals are more or less normally distributed and check the residuals by predicted values plot.

Note that the linearity assumption also applies to the interaction effect. If the interaction effect is positive, the exposure (predictor) effect must be higher for higher values of contact with smokers (moderator). More precisely, a unit difference on the moderator should result in a fixed increase (or decrease) of the effect of the predictor. You may have noticed this linear change in the effect size in Figure \@ref(fig:continuous-moderator) at the beginning of this section on continuous moderators.

It is difficult to check this assumption, so let us not pursue this here. Just remember that the interaction effect is assumed to be linear: an ever increasing or decreasing effect of the predictor at higher moderator values.

```{r eval=FALSE, echo=FALSE}
### Comparing nested regression models

Discuss F Change test here with distinction between 'main' effects in model without interaction predictor and conditional effects in (nested?) model with interaction predictor ; additional SPSS clip?
Cf. Fam: Discuss a two-step approach to moderation? In the first model estimate effects without the interaction predictor, so we have the average or main effects (as in ANOVA). In the second model, add the interaction predictor. Now, the former main effect is the effect for the reference group or value (zero) on the moderator.
Pros/cons: Adds F Change test; F Change test does not add to significance test of interaction predictor?; highlights interpretation difference of seemingly the same effect (main effect becomes conditional effect); main effects are interesting only if there is no interaction effect?
```

### Higher-order interaction effects

An interaction effect with one moderator, whether continuous or categorical, is called _first-order interaction_. It is possible to have a moderated effect that is moderated itself by a second moderator. For example, the change in the exposure effect due to a person's contact with smokers may be different for smokers than for non-smokers. This is called a _second-order interaction_ or _higher-order interaction_. We can include more moderators, yielding even higher higher-order interactions, such as three or four moderators.

An interaction variable that is the product of the predictor and two moderators can be used to include a second-order interaction in a regression model. If you include a second-order interaction, you must also include the effects of the variables involved in the interaction as well as all first-order interactions among these variables in the regression model. All in all, these models become very complicated to interpret, so we do not pay attention to them.

## Reporting Regression Results with Moderation {#reportmoderation}

```{r report-moderation-table, echo=FALSE}
# Generate data with categorical*continuous and continuous*continuous moderation.
# Number of observations.
n <- 150
# Create predictors
set.seed(4932)
exposure <- runif(n)*10
set.seed(823)
former <- rbinom(n, 1, 0.40)
set.seed(401)
smoker <- rbinom(n, 1, 0.20)
smoker[former == 1] <- 0
set.seed(4321)
contact <- 0.12*(10 - exposure) + rnorm(n, mean = 4.5, sd = 2)
# Mean-centered predictors.
exposure_mc <- exposure - mean(exposure)
contact_mc <- contact - mean(contact)
# Create outcome for mean-centered numeric predictor and moderator.
set.seed(390)
attitude <- -0.26*exposure_mc + 0.25*contact_mc + 0.08*exposure_mc*contact_mc - 1.6*former + 0.06*smoker - 0.12*former*exposure_mc + 0.05*smoker*exposure_mc + rnorm(n, mean = -1, sd = 1)
# Regression.
regmodel_1 <- lm(attitude ~ exposure_mc*contact_mc + exposure_mc*former + exposure_mc*smoker)
# Collect model test results.
summ <- summary(regmodel_1)
resultsF <- cbind(c("1", "", ""),
                  c("Regression", "Residual", "Total"),
                  c(format(round(var(attitude)*(n-1) - sum(summ$residuals^2), digits = 3), nsmall = 3), 
                    format(round(sum(summ$residuals^2), digits = 3), nsmall = 3),
                    format(round(var(attitude)*(n-1), digits = 3), nsmall = 3)),
                  c(round(summ$fstatistic[2]), round(summ$fstatistic[3]), n - 1),
                  c(format(round((var(attitude)*(n-1) - sum(summ$residuals^2))/summ$fstatistic[2],digits=3), nsmall = 3), format(round((var(attitude)*(n-1))/summ$fstatistic[3],digits=3), nsmall = 3), ""),
                  c(format(round(summ$fstatistic[1], digits = 3), nsmall = 3), "", ""),
                  c(format(round(pf(summ$fstatistic[1], summ$fstatistic[2], summ$fstatistic[3], lower.tail = FALSE), digits = 3), nsmall = 3), "", "")
                )
# Table with coefficient results in SPSS style.
results <- coef(summary(regmodel_1))
# Confidence intervals
ci <- confint.lm(regmodel_1)
# Reorder for APA6 table.
table <- cbind(paste0(format(round(results[,1], digits=2), nsmall=2),
                      ifelse(results[,4] < 0.001, "***", 
                        ifelse(results[,4] < 0.01, "**",
                          ifelse(results[,4] < 0.05, "*", "")))),
                 paste0("[", format(round(ci[,1], digits=2), nsmall = 2),
                        ", ", 
                        format(round(ci[,2], digits=2), nsmall = 2), "]"))
# Add R2 and F
table <- rbind(table, c(format(round(summ$r.squared, digits=2), nsmall = 2), ""),
               c(paste0(format(round(summ$fstatistic[1], digits=2), nsmall=2),
                      ifelse(pf(summ$fstatistic[1], summ$fstatistic[2], summ$fstatistic[3], lower.tail = FALSE) < 0.001, "***", 
                        ifelse(pf(summ$fstatistic[1], summ$fstatistic[2], summ$fstatistic[3], lower.tail = FALSE) < 0.01, "**",
                          ifelse(pf(summ$fstatistic[1], summ$fstatistic[2], summ$fstatistic[3], lower.tail = FALSE) < 0.05, "*", "")))), ""))
# Adjust parameter names
rownames(table) <- c("Constant", "Exposure", "Contact", "Former smoker", "Smoker", "Exposure * Contact", "Exposure * Former smoker", "Exposure * Smoker", "R^2^", "F")
attributes(table)$dimnames[[1]][1] <- "Constant"
attributes(results)$dimnames[[1]][6] <- "exposure*contact"
attributes(results)$dimnames[[1]][7] <- "exposure*former smoker"
attributes(results)$dimnames[[1]][8] <- "exposure*smoker"
# Set column names.
colnames(table) <- c("B", "95% CI")
# Table.
options(knitr.kable.NA = '')
knitr::kable(table, align = c("l", "c"), caption = "Predicting attitude towards smoking. Results in APA6 style. Exposure and contact are mean-centered.")# Helper function for displaying results within the text.
source("report_n.R")
```

_Note_. _N_ = `r n`. CI = confidence interval.

\* _p_ < .05. \** _p_ < .01. \*** _p_ < .001.

If we report a regression model, we first present the significance test and predictive power of the entire regression model. We may report that the regression model is statistically significant, F (`r resultsF[[1,4]]`, `r resultsF[[2,4]]`) = `r report_n(as.numeric(resultsF[[1,6]]),2)`, p `r ifelse(resultsF[[1,7]] == "0.000", "< 0.001", paste0("=", resultsF[[1,7]]))`, so the regression model very likely helps to predict attitude towards smoking in the population. Retrieve the test information from SPSS; the APA6-style table (Table \@ref(tab:report-moderation-table)) only reports the F value and its significance level.

How well does the regression model predict attitude towards smoking? The effect size of a regression model or its predictive power is summarized by $R^2$ (_R Square_), which is the proportion of the variation in the outcome variable scores (attitude towards smoking) that can be predicted with the regression model. In this example, $R^2$ is `r report_n(summ$r.squared, 2)`, so the regression model predicts `r report_n(summ$r.squared * 100, 0)`% of the variance in attitude towards smoking among the respondents. In communication research, $R^2$ is usually smaller. 

$R^2$ tells us how well the regression model predicts the outcome variable in the sample. Every predictor that we add to the regression model helps to predict results in the sample even if the predictor does not help to predict the outcome in the population. For a better idea of the predictive power of the regression model in the population, we may use _Adjusted R Square_. Adjusted R Square is usually slightly lower than R Square. In the example, Adjusted R Square is `r report_n(summ$adj.r.squared, 2)` (not reported in Table \@ref(tab:report-moderation-table)).

As a next step, we discuss the size, statistical significance, and confidence intervals of the regression coefficients. If a predictor is involved in one or more interaction effects, we must be very clear about the reference value and reference group to which the effect applies.

Exposure, in our example, has a negative predictive effect on attitude towards smoking for non-smokers with average contacts with smokers, t = `r report_n(results[2,3])`, `r ifelse(results[2,4] < .0005, "p < .001", paste0("p = ", report_n(results[2,4], digits=3)))`, 95%CI[`r report_n(ci[2,1])`, `r report_n(ci[2,2])`]. Note that SPSS does not report the degrees of freedom for the t test on  regression coefficient, so we cannot report them.

Instead of presenting the numerical results in the text, we may summarize them in an APA6 style table, such as Table \@ref(tab:report-moderation-table). Note that t and p values are not reported in this table, the focus is on the confidence intervals. The significance level is indicated by stars.

A sizable and statistically significant interaction effect signals that an effect is moderated. In the example reported in Table \@ref(tab:report-moderation-table), the effect of exposure on attitude seems to be moderated by contact with smokers (_b_ = `r report_n(results[6,1])`, `r ifelse(results[6,4] < .0005, "p < .001", paste0("p = ", report_n(results[6,4], digits=3)))`) and by smoking status (_b_ = `r report_n(results[7,1])`, `r ifelse(results[7,4] < .0005, "p < 0.001", paste0("p = ", report_n(results[7,4], digits=3)))`). 

The regression coefficients for interaction effects must be interpreted as effect differences. For a categorical moderator, the coefficient describes the effect size difference between the category represented by the dummy variable and the reference group. The negative effect of exposure is stronger for former smokers than for the reference group non-smokers. The average difference is `r report_n(results[7,1])`.

For a continuous moderator, we can interpret the general pattern reflected by the interaction effect. A positive interaction effect, such as `r report_n(results[6, 1])` for the interaction between exposure and smoker contact, signals that the effect of exposure is more strongly positive or less negative at higher levels of contact with smokers. 

This interpretation in terms of effect differences remains difficult to understand. It is recommended to select some interesting values for the moderator and report the size of the effect for each value. For a categorical moderator, each category is of interest. For a continuous moderator, the mean and one standard deviation below and above the mean are usually interesting values. The regression coefficients show whether the effect is positive, negative, or nearly zero at different values of the moderator.

Visualize the regression lines for different values of the moderator rather than presenting the numerical results. If the regression model contains covariates, mention the values that you have used for the covariates. Select one of the categories for a categorical covariate. For numeric covariates, the mean is a good choice. If you are working with mean-centered predictors, be sure to use the mean-centered predictor for the horizontal axis (as in Figure \@ref(fig:report-moderator-visual)), not the original predictor.

```{r report-moderator-visual, fig.cap="The effect of exposure on attitude towards smoking. Left: Effects for groups with different smoking status (at average contact with smokers). Right: Effects at different levels of contact with smokers (effects for non-smokers).", out.width='50%', fig.asp=1, fig.show='hold', echo=FALSE}
# Create grouping variable. 
status <- rep(0, n)
status[former == 1] <- 1
status[smoker == 1] <- 2
status <- factor(status, labels = c("non-smoker", "former smoker", "smoker"))
df <- data.frame(attitude, contact, exposure, former, smoker, status)
ggplot(df, aes(x = exposure_mc, y = attitude, colour = status)) +
  geom_point(size = 4) +
  geom_abline(slope = results[2,1], intercept = results[1,1], colour = "red", size = 1) + #nonsmoker
  geom_abline(slope = (results[2,1] + results[7,1]), intercept = (results[1,1] + results[4,1]), colour = "green", size = 1) + #former smoker
  geom_abline(slope = (results[2,1] + results[8,1]), intercept = (results[1,1] + results[5,1]), colour = "blue", size = 1) + #smoker
  theme_classic(base_size = 18) +
  xlab("Exposure (mean-centered)") +
  ylab("Attitude towards smoking") +
  theme(legend.position = "bottom")
# define colours.
cl <- RColorBrewer::brewer.pal(5, "Blues")
ggplot(df, aes(x = exposure_mc, y = attitude, colour = contact)) +
  geom_point(size = 4) +
  geom_abline(slope = (results[2,1] + results[6,1]*sd(contact)), intercept = (results[1,1] + results[3,1]*(sd(contact))), colour = cl[3], size = 1) +
  geom_abline(slope = (results[2,1]), intercept = (results[1,1]), colour = cl[4], size = 1)  +
  geom_abline(slope = (results[2,1] - results[6,1]*sd(contact)), intercept = (results[1,1] - results[3,1]*sd(contact)), colour = cl[5], size = 1) +
  theme_classic(base_size = 18) +
  xlab("Exposure (mean-centered)") +
  ylab("Attitude towards smoking") +
  scale_color_continuous(breaks = c(mean(contact)-sd(contact), mean(contact), mean(contact)+sd(contact)), labels = c("M-SD", "M", "M+SD")) +
  theme(legend.position = "bottom", legend.key.size = unit(1.6, "cm") )

# It is possible to translate the regression equation for a mean-centered predictor back to the original scale of the predictor. The simple slope (of the conditional effects) remains the same. The intercept has to be adjusted: It is the intercept estimated for the mean-centered predictor minus the mean of the original predictor times the slope. Graphically speaking, the intercept must be moved from the mean of the original predictor, which is zero on the mean-centered predictor) to zero on the original predictor, which is minus the original mean on the mean-centered predictor. The inercept with the original predictor, then, is M steps to the left from zero on the regressio line for the mean-centered predictor.
# ggplot(df, aes(x = exposure, y = attitude, colour = status)) +
#   geom_point(size = 4) +
#   geom_abline(slope = results[2,1], intercept = (results[1,1] - results[2,1]*mean(exposure)), colour = "red", size = 1) + #nonsmoker
#   geom_abline(slope = (results[2,1] + results[7,1]), intercept = (results[1,1] + results[4,1] - (results[2,1] + results[7,1])*mean(exposure)), colour = "green", size = 1) + #former smoker
#   geom_abline(slope = (results[2,1] + results[8,1]), intercept = (results[1,1] + results[5,1] - (results[2,1] + results[8,1])*mean(exposure)), colour = "blue", size = 1) + #smoker
#   theme_classic(base_size = 18) +
#   xlab("Exposure") +
#   ylab("Attitude towards smoking") +
#   theme(legend.position = "bottom")
# ggplot(df, aes(x = exposure, y = attitude, colour = contact)) +
#   geom_point(size = 4) +
#   geom_abline(slope = (results[2,1] + results[6,1]*sd(contact)), intercept = (results[1,1] + results[3,1]*(sd(contact)) - (results[2,1] +  results[6,1]*sd(contact))*mean(exposure)), colour = cl[3], size = 1) + #M + SD
#   geom_abline(slope = (results[2,1]), intercept = (results[1,1] - results[2,1]*mean(exposure)), colour = cl[4], size = 1)  + #M
#   geom_abline(slope = (results[2,1] - results[6,1]*sd(contact)), intercept = (results[1,1] - results[3,1]*sd(contact) - (results[2,1] - results[6,1]*sd(contact))*mean(exposure)), colour = cl[5], size = 1) +
#   theme_classic(base_size = 18) + #M - SD
#   xlab("Exposure") +
#   ylab("Attitude towards smoking") +
#   scale_color_continuous(breaks = c(mean(contact)-sd(contact), mean(contact), mean(contact)+sd(contact)), labels = c("M-SD", "M", "M+SD")) +
#   theme(legend.position = "bottom", legend.key.size = unit(1.6, "cm") )

#Cleanup.
rm(ci, df, results, resultsF, table, attitude, cl, contact, exposure, former, n, regmodel_1, smoker, status, summ, report_n)
```

The left panel in Figure \@ref(fig:report-moderator-visual) clearly shows that the effect of exposure on attitude is more or less the same for non-smokers and smokers. The effect is different for former smokers, for whom the exposure effect is more strongly negative. It is difficult to draw this conclusion from the table with regression coefficients.

Check that the predictor has good support at the selected values of the moderator. In the left-hand plot of Figure \@ref(fig:report-moderator-visual), the groups (colours) vary nicely over the entire range of the predictor _exposure_, so that is okay. It is more difficult to see good variation in the right-hand plot. 

Do not report that common support is good. If it is bad, warn the reader that we cannot fully trust the estimated moderation because we do not have a nice range of predictor values within each level of the moderator.

Finally, inspect the residual plots but do not include them in the report. Warn the reader if the assumptions of the linear regression model are not met. Do not mention the assumptions if they are met.

## A Continuous Moderator in SPSS {#RegressionContModSPSS}

### Instructions

```{r SPSSregcenter, echo=FALSE, out.width="640px", fig.cap="(ref:regcenterSPSS)", dev="png", screenshot.opts = list(delay = 5)}
knitr::include_url("https://www.youtube.com/embed/2947blS-Dnc", height = "360px")
# Goal: Mean-centering numeric variables (both predictor and moderator) in SPSS.
# Example: smokers.sav, the effect of campaign exposure on attitude towards smoking moderated by contacts that people have with smokers.
# SPSS menu: 
#  1. determine average score on a variable: Analyze > Descriptive Statistics > Frequencies ; select Statistics > Mean (and Minimum, Maximum to check range) and unselect Display frequency tables
#  2. create a new variable with the average subtracted: Transform > Compute, select variable, give new name (indicating centering), and subtract value of average from Frequencies output
#  3. Calculate the interaction predictor from the two mean-centered variables.
# Inspect output: descriptives (and unstandardized coefficients) in regression analysis ;  never mind rounding errors or differences due to listwise deletion of missing values)
```

----

```{r SPSSreglines2, echo=FALSE, out.width="640px", fig.cap="(ref:reglines2SPSS)", dev="png", screenshot.opts = list(delay = 5)}
knitr::include_url("https://www.youtube.com/embed/8jnrg7nKsuk", height = "360px")
# Goal: Graph regression lines for different moderator values in a scatterplot.
# Example: smokers.sav, the effect of campaign exposure on attitude towards smoking moderated by contacts that people have with smokers (both mean-centered), smoking status as covariate.
# Techniques: using reference lines for M, M - SD, and M + SD ; with mean-centered moderator, add SD to obtain reference line for M - SD and M + SD.
# SPSS menu: {after having applied} regression analysis with descriptives, reconstruct regression equation, calculate mean of moderator minus one SD and plug into the equation ; use mean or reference value for covariat(s) ; Graphs > Legacy Dialogs > Scatter/Dot > Simple Scatter, in the Chart Editor, add reference line (Options > Reference Line from Equation)
# Interpret results. 
```

----

```{r SPSSregSupport2, echo=FALSE, out.width="640px", fig.cap="(ref:regSupport2SPSS)", dev="png", screenshot.opts = list(delay = 5)}
knitr::include_url("https://www.youtube.com/embed/_OwIfQhOAxU", height = "360px")
# Goal: Checking common support with a continuous moderator; group moderator in 3 groups (terciles) and create (panelled) histograms for the predictor scores in each moderator group
# Example: smokers.sav, the effect of campaign exposure on attitude towards smoking moderated by contacts that people have with smokers.
# SPSS menu: Transform > Visual Binning
# Interpret output: 

# : (for enthusiasts?) don't interpret the standardized regression coefficients (Beta) for interaction variables in SPSS because they are calculated in the wrong way ; the predictor and moderator variables are multiplied to obtain the interaction variable and aferwards they are standardized ; instead, the predictor and moderator variables should be standardized before they are multiplied ; if you want to interpret the standardized regression coefficients, you have to standardize _all_ numeric variables yourself (Analyze > Descriptive Statistics > Descriptives with option 'Save standardized values as variables' checked) before you calculate the interaction variable and include them in the regression analysis ; in this situation, the output of the regression analysis lists the standardized regression weights in the column 'Unstandardized Coefficients'. 
```

### Exercises

1. With the data in [smokers.sav](http://82.196.4.233:3838/data/smokers.sav), check if the effect of campaign exposure on attitude towards smoking depends on the contacts that people have with smokers. For now, do not mean-center the variables. Control for the respondent's smoking status (_status2_). Interpret the regression coefficients and check the assumptions of the regression model.

```{r eval=FALSE}
SPSS syntax:  
  
* Check data.  
FREQUENCIES VARIABLES=exposure status2 contact attitude  
  /ORDER=ANALYSIS.  
* Compute interaction variable.  
COMPUTE expo_contact=exposure * contact.  
VARIABLE LABELS  expo_contact 'Interaction exposure * contact'.  
EXECUTE.  
* Multiple regression.  
* Statistic Descriptives is added to get the means that we need  
* to plug into the regression equation in the moderation plot.  
REGRESSION  
  /DESCRIPTIVES MEAN STDDEV CORR SIG N  
  /MISSING LISTWISE  
  /STATISTICS COEFF OUTS CI(95) R ANOVA  
  /CRITERIA=PIN(.05) POUT(.10)  
  /NOORIGIN   
  /DEPENDENT attitude  
  /METHOD=ENTER exposure contact expo_contact status2  
  /SCATTERPLOT=(*ZRESID ,*ZPRED)  
  /RESIDUALS HISTOGRAM(ZRESID).  
  
Check data:  
  
There are no impossible values on the variables.  
  
Check assumptions:  
  
* The residuals are skewed, so the assumption of a normal distribution can be
violated.
* The residuals seem to average to zero at all levels of the predicted
outcome. This supports a linear model. Note that it is not a problem that the
residuals tend to be further from zero if they are below zero.
* Prediction errors seem to be more or less of equal size at different levels
of the outcome variable, so the assumpion of homoscedasticity seems to be met.
  
Interpret the results:  
  
* The regression model predicts 21 per cent of the variation in the outcome
variable, F (4, 80) = 5.44, p = .001.
* None of the regression coefficients, however, is statistically significant.
We are not confident that the directions of the estimated effects are the true
directions; they may have the opposite direction in the population.
* But we should realize that the significance tests of the moderated partial
effects apply to the effect at one particular level of the moderator. At this
level of the moderator, the estimated coefficient is not sufficiently
different from zero. At other levels of the moderator, however, the
coefficient can be sufficiently different from zero for the test to be
statistically significance.
  
* The effects of exposure and contact with smokers must be interpreted with
care due to their interaction effect.
* The estimated effect of exposure applies to adults who score zero on the
variable contact with smokers. In this context, exposure makes the predicted
attitude towards smoking more negative, b = -0.27, t = -1.54, p = .128.
* Contact with smokers makes the attitude more positive for adults who have no
exposure to the anti-smoking campaign, b = 0.07, t = 0.41, p = .682.
* The interaction effect is positive, b = .02, t = 0.53, p = .595, so exposure
seems to be less effective in making the attitude towards smoking more
negative if the adult has more contacts with smokers.
```

2. Visualize the moderating effect of contact with smokers on the exposure effect in a scatter plot with three regression lines. Explain the information conveyed by the plot to your reader

```{r eval=FALSE}
SPSS syntax:  
  
* Create scatterplot.  
GRAPH  
  /SCATTERPLOT(BIVAR)=exposure WITH attitude  
  /MISSING=LISTWISE.  
  
Manually add three regression lines:  
  
* Write out the regression equations for different values of the moderator.
* Plug in the estimated values of the regression coefficients, the means of
covariates, and three values for the moderator using its M and SD.
  
The initial equation for non-smokers (status = 0):  
  
attitude = .648*constant + -.265*exposure + .072*contact + 
  .018*exposure*contact + 0.533*status  
  
attitude = .648*1 + -.265*exposure + .072*contact + 
  .018*exposure*contact + 0.533*0  
  
attitude = .648 + (-.265 + .018*contact)*exposure + .072*contact  
  
The equation with Contact = M - SD  
  
attitude = .648 + (-.265 + .018*(5.091 - 1.974))*exposure + .072*(5.091 - 1.974)  
  
attitude = .648 + (-.265 + .018*3.117)*exposure + .072*3.117  
  
attitude = .648 + (-.265 + .056)*exposure + .224  
  
attitude = .872 + -.209*exposure  
  
The equation with Contact = M  
  
attitude = .648 + (-.265 + .018*5.091)*exposure + .072*5.091  
  
attitude = .648 + (-.265 + .092)*exposure + .367  
  
attitude = 1.015 + -.173*exposure  
  
The equation with Contact = M + SD  
  
attitude = .648 + (-.265 + .018*(5.091 + 1.974))*exposure + .072*(5.091 + 1.974)  
  
attitude = .648 + (-.265 + .018*7.065)*exposure + .072*7.065  
  
attitude = .648 + (-.265 + .127)*exposure + .509  
  
attitude = 1.157 + -.138*exposure  
  
Interpret the results:  
    
* The negative predictive effect of exposure on attitude towards smoking is
slightly stronger (more negative) for adults with fewer contacts with smokers.
```

3. Mean-center the predictor and moderator and repeat the regression analysis of Exercise 1. Explain the differences in the results.

```{r eval=FALSE}
SPSS syntax:  
  
* Check data.  
FREQUENCIES VARIABLES=exposure status2 contact attitude  
  /ORDER=ANALYSIS.  
* Mean-center predictor and moderator.  
* Ask for means of predictor and exposure.  
FREQUENCIES VARIABLES=exposure contact  
  /FORMAT=NOTABLE  
  /STATISTICS=MEAN  
  /ORDER=ANALYSIS.  
* Subtract mean from variable.  
COMPUTE exposure_c=exposure - 4.866.  
VARIABLE LABELS  exposure_c 'Exposure (mean-centered)'.  
COMPUTE contact_c=contact - 5.091.  
VARIABLE LABELS  contact_c 'Contact (mean-centered)'.  
EXECUTE.  
* Compute new interaction variable.  
COMPUTE expo_contact_c=exposure_c * contact_c.  
VARIABLE LABELS  expo_contact_c 'Interaction exposure * contact  (mean-centered)'.  
EXECUTE.  
* Multiple regression.  
* Statistic Descriptives is added to get the means that we need  
* to plug into the regression equation in the moderation plot.  
REGRESSION  
  /DESCRIPTIVES MEAN STDDEV CORR SIG N  
  /MISSING LISTWISE  
  /STATISTICS COEFF OUTS CI(95) R ANOVA  
  /CRITERIA=PIN(.05) POUT(.10)  
  /NOORIGIN   
  /DEPENDENT attitude  
  /METHOD=ENTER exposure_c contact_c expo_contact_c status2  
  /SCATTERPLOT=(*ZRESID ,*ZPRED)  
  /RESIDUALS HISTOGRAM(ZRESID).  
  
Check data: See Exercise 1.  
  
Check assumptions: See Exercise 1.  
  
Interpret the results:  
  
* The size and significance of the interaction effect have not changed at all.
Mean-centering only changes the reference values for the effects of the
predictor and the moderator.
* The coefficient for exposure now expresses the predictive effect of exposure
for adults with average contact with smokers. They have more contact with
smokers than the reference group in Exercise 1, who had no contact with
smokers. The interaction effect tells us that the effect of exposure becomes
less negative at higher levels of contact. This explains that we have a lower
value for the exposure coefficient now. It still is negative, so more exposure
to the anti-smoking campaign predicts a more negative attitude towards
smoking for adults with average contact with smokers.
* In contrast, the positive effect of contact with smokers on attitude is
stronger now (b = 0.16) than in Exercise 1 (b = 0.07). The interaction effect
tells us that contact has a more positive effect on smoking attitude for
higher levels of campaign exposure. As a result, the effect of contact at
average exposure is stronger than at zero exposure.
  
* Why do we have a statistical significant result for the effect of exposure
now but not in Exercise 1?
* The size of the unstandardized effect is lower (b = -0.17) now than in
Exercise 1 (b = -0.27). It is closer to zero so we would not expect
statistical significance. However, the standard error is much smaller now: SE
= 0.06 against SE = 0.17. We have quite some observations with about average
contact score (the reference value if we mean-center) but hardly any
observations with minimum (zero) contact score. With fewer observations, we
are less certain about estimates, so we have a larger standard error, and it
is more difficult to be confident that the rergession coefficient is not zero
in the population.
```

4. Check common support of the predictor for the moderator. Divide the moderator into three groups.

```{r eval=FALSE}
SPSS syntax:  
  
* Group the moderator.  
* Visual Binning.  
*contact.  
RECODE  contact (MISSING=COPY)  
  (LO THRU 4.25076386584132=1)  
  (LO THRU 5.83711577142397=2)   
  (LO THRU HI=3) (ELSE=SYSMIS) INTO contact_3.  
VARIABLE LABELS  contact_3 'Contact with smokers (Binned)'.  
FORMATS  contact_3 (F5.0).  
VALUE LABELS  contact_3 1 '' 2 '' 3 ''.  
VARIABLE LEVEL  contact_3 (ORDINAL).  
EXECUTE.  
* Histograms of the predictor for each moderator group.  
GRAPH  
  /HISTOGRAM=exposure  
  /PANEL ROWVAR=contact_3 ROWOP=CROSS.  
  
Interpret the results:  

* Coverage of the exposure predictor is poor at low contact levels. Especially
low exposure hardly occurs at low contact level. This explains the high
standard error for the exposure effect in Exercise 1 as explained in Exercise
3.
```

5. Let us hypothesize that children's media literacy depends on sex, age, and parental supervision. Is the effect of parental supervision moderated by the child's age?
    Use [children.sav](http://82.196.4.233:3838/data/children.sav) to answer this research question and apply mean-centering. 
    Report the results as required in this course (APA6), include a moderation plot, and discuss coverage. 

```{r eval=FALSE}
SPSS syntax:  
    
* Check data.  
FREQUENCIES VARIABLES=medliter sex age supervision  
  /STATISTICS=MEAN  
  /ORDER=ANALYSIS.  
* Set impossible values to missing.  
* Define Variable Properties.  
*sex.  
MISSING VALUES sex(1).  
*supervision.  
MISSING VALUES supervision(25.00).  
EXECUTE.  
* Turn sex into a 0/1 variable.  
RECODE sex (2=0) (3=1) INTO girl.  
VARIABLE LABELS  girl 'The child is a girl.'.  
EXECUTE.  
* Mean-center predictor and moderator.  
* Ask for means of predictor and exposure.  
FREQUENCIES VARIABLES=age supervision  
  /FORMAT=NOTABLE  
  /STATISTICS=MEAN  
  /ORDER=ANALYSIS.  
* Subtract mean from variable.  
COMPUTE age_c=age - 8.609.  
VARIABLE LABELS  age_c 'Age (mean-centered)'.  
COMPUTE supervision_c=supervision - 5.358.  
VARIABLE LABELS  supervision_c 'Supervision (mean-centered)'.  
EXECUTE.  
* Check mean centering.  
FREQUENCIES VARIABLES=age_c supervision_c  
  /FORMAT=NOTABLE  
  /STATISTICS=MEAN  
  /ORDER=ANALYSIS.  
* Compute interaction variable.  
COMPUTE age_supervision_c=age_c * supervision_c.  
VARIABLE LABELS  age_supervision_c 'Interaction age * supervision (mean-centered)'.  
EXECUTE.  
* Multiple regression.  
* Statistic Descriptives is added to get the means that we need  
* to plug into the regression equation in the moderation plot.  
REGRESSION  
  /DESCRIPTIVES MEAN STDDEV CORR SIG N  
  /MISSING LISTWISE  
  /STATISTICS COEFF OUTS CI(95) R ANOVA  
  /CRITERIA=PIN(.05) POUT(.10)  
  /NOORIGIN   
  /DEPENDENT medliter  
  /METHOD=ENTER girl age_c supervision_c age_supervision_c  
  /SCATTERPLOT=(*ZRESID ,*ZPRED)  
  /RESIDUALS HISTOGRAM(ZRESID).  
* Create scatterplot for moderation plot.  
* Use the mean-centered variable.  
GRAPH  
  /SCATTERPLOT(BIVAR)=supervision_c WITH medliter  
  /MISSING=LISTWISE.  
* Manually add three regression lines.  
    
Check data:  
  
* Score '25' for parental supervision cannot be right because the scale runs
to 10. Define this score as a missing value.
* The sex category '1' cannot be right either.  
  
Check assumptions:  
  
* The residuals are quite normally distributed, as they should.  
* The residuals are centered around zero for all levels of the predicted
outcome (linearity) but the variation in residuals seems to be a bit larger at
higher predicted values (the residuals may not be homoscedastic).
  
Interpret the results:  
    
* The regression model predicts 19 per cent of the differences in media
literacy among children, F (4, 80) = 4.58, p = .002.
* There is no remarkable difference between girls and boys, t = 0.58, p =
.566, 95%CI[-0.47; 0.86].
* Girls may be upto 0.86 more media literate on average than boys but we
cannot rule out that boys have on average more media literacy (up to 0.47).
* Age has a statistically significant positive effect on media literacy for
children at average parental supervision, t = 2.02, p = .047, 95%CI[0.003;
0.35].
* Parental supervision has a positive effect on media literacy, t = 3.36, p =
.001, 95%CI[0.12; 0.47].
* There is no statistically significant interaction effect between age and
parental supervision on media literacy, t = .50, p =.615, 95%CI[-0.07; 0.12].
If there is an interaction effect in the population, it can be negative nearly
as well as positive.

* Write out the regression equations for different values of the moderator.
* Plug in the estimated values of the regression coefficients, the selected
category of the covariate, and three values for the moderator using its M and
SD.
* Create regression lines for the effect of media literacy at three levels of
parental supervision (M - SD, M, and M + SD) in the scatterplot of media
literacy by mean-centered parental supervision.
  
Estimated regression equation:   
    
medliter = 4.325 + 0.193 * girl + 0.176 * age_centered + 0.292 * supervision_centered + 
  0.025 * age_c * supervision_c  
  
With rearranged terms and sex plugged in for boys:  
  
medliter = 4.325 + 0.193 * 0 + 0.176 * age_centered + 
  (0.292  + 0.025 * age_c) * supervision_centered  
  
Age at M - SD (mean-centered so M = 0, SD = 1.937) for boys:  
  
medliter = 4.325 + 0.176*(0 - 1.937) + (0.292 + 0.025*(0 - 1.937))*supervision  
  
medliter = 4.325 + -0.341 + (0.292 + -0.048)*supervision  
  
medliter = 3.984 + 0.244*supervision  
    
Age at M (M = 0) for boys:  
  
medliter = 4.325 + .176*(0) + (0.292 + .025*(0))*supervision  
  
medliter = 4.325 + 0.292*supervision  
  
Age at M + SD (M = 0, SD = 1.937) for boys:  
  
medliter = 4.325 + .176*(0 + 1.937) + (0.292 + 0.025*(0 + 1.937))*supervision  
  
medliter = 4.325 + 0.341 + (0.292 + 0.048)*supervision  
  
medliter = 4.666 + 0.340*supervision  
  
Note: Age was mean-centered for all 87 cases but the regression model only
uses the 85 cases without a missing value on any of the variables. As a
result, the mean of age scores in the regression model is not exactly zero. We
can still use zero here because we merely want to refer to an age value that
is in the center of the distribution. Both zero and nearly zero are in the
center.  

* The regression lines in the moderation plot have quite similar slopes, which
illustrates the absence of a substantial interaction effect.  
```

6. Is the effect of parental supervision moderated by sex? Use the data of Exercise 5 to answer this question. You may omit the age predictor from the model. Again, illustrate your answer with a moderation plot.

```{r eval=FALSE}
SPSS syntax:  
  
* Check data.  
FREQUENCIES VARIABLES=medliter sex supervision  
  /STATISTICS=MEAN  
  /ORDER=ANALYSIS.  
* Set impossible values to missing.  
* Define Variable Properties.  
*sex.  
MISSING VALUES sex(1).  
*supervision.  
MISSING VALUES supervision(25.00).  
EXECUTE.  
* Turn sex into a 0/1 variable.  
RECODE sex (2=0) (3=1) INTO girl.  
VARIABLE LABELS  girl 'The child is a girl.'.  
EXECUTE.  
* Mean-center the predictor.  
* Ask for means of parental supervision.  
FREQUENCIES VARIABLES=supervision  
  /FORMAT=NOTABLE  
  /STATISTICS=MEAN  
  /ORDER=ANALYSIS.  
* Subtract mean from variable.  
COMPUTE supervision_c=supervision - 5.358.  
VARIABLE LABELS  supervision_c 'Supervision (mean-centered)'.  
EXECUTE.  
* Compute interaction variable.  
COMPUTE girl_supervision_c=girl * supervision_c.  
VARIABLE LABELS  girl_supervision_c 'Interaction girl * supervision (mean-centered)'.  
EXECUTE.  
* Multiple regression.  
* Statistic Descriptives is added to get the means that we need  
* to plug into the regression equation in the moderation plot.  
REGRESSION  
  /DESCRIPTIVES MEAN STDDEV CORR SIG N  
  /MISSING LISTWISE  
  /STATISTICS COEFF OUTS CI(95) R ANOVA  
  /CRITERIA=PIN(.05) POUT(.10)  
  /NOORIGIN   
  /DEPENDENT medliter  
  /METHOD=ENTER girl supervision_c girl_supervision_c  
  /SCATTERPLOT=(*ZRESID ,*ZPRED)  
  /RESIDUALS HISTOGRAM(ZRESID).  
* Scatterplot with dots coloured by sex.  
* Use the mean-centered predictor.  
GRAPH  
  /SCATTERPLOT(BIVAR)=supervision_c WITH medliter BY girl  
  /MISSING=LISTWISE.  
* Note: This model does not contain a covariate, so SPSS can draw the lines.  
* Provided that your SPSS installation contains the command:
* Graphs > Regression Variable Plots; Color by: sex..  
* With options: Scatterplot Fit Lines: Linear, 
*    Grouping: Fit Line for each categorical colour group.  
* Use the mean-centered or not mean-centered predictor.  
STATS REGRESS PLOT YVARS=medliter XVARS=supervision_c COLOR=sex   
/OPTIONS CATEGORICAL=BARS GROUP=1 INDENT=15 YSCALE=75   
/FITLINES LINEAR APPLYTO=GROUP.  
  
Check data: See Exercise 5.  
  
Check assumptions:  
  
As with the regression model in Exercise 5.  
* The residuals are quite normally distributed and centered around zero for
all levels of the predicted outcome (linearity).
* The variation in residuals seems to be a bit larger at higher predicted
values (the residuals may not be homoscedastic).
  
Interpret the results:  
  
* The regression model predicts 15 per cent of the variation in outcome
scores, F (3, 81) = 4.76, p = .004.
* There is no remarkable difference between girls and boys, t = 0.37, p =
.711, 95%CI[-0.55; 0.80] for children at average supervision level. Girls may
have up to 0.80 more media literacy on average than boys but we cannot rule
out that boys have on average more media literacy (up to 0.55).
* Parental supervision has a statistically significant positive effect on
media literacy for boys, b = 0.39, t = 2.99, p = .004, 95%CI[0.13; 0.65] that
is weaker than for girls, b = 0.26 but the difference between boys and girls
is not statistically significant, t = -0.76, p = .448, 95%CI[-0.48; 0.22].

* If you add the regression lines for boys and girls manually, use the
mean-centered supervision variable and the following equations:
  
Equation for boys (girl = 0):  
  
medliter = 4.374 + 0.125*girl + 0.389*supervision + -0.134*girl*supervision  
  
medliter = 4.374 + 0.125*0 + (0.389 + -0.134*girl)*supervision  
  
medliter = 4.374 + (0.389 + -0.134*0)*supervision  
  
medliter = 4.374 + 0.389*supervision  
  
Equation for girls (girl = 1):  
  
medliter = 4.374 + 0.125*girl + 0.389*supervision + -0.134*girl*supervision  
  
medliter = 4.374 + 0.125*1 + (0.389 + -0.134*girl)*supervision  
  
medliter = 4.499 + (0.389 + -0.134*1)*supervision  
  
medliter = 4.499 + 0.255*supervision  
```

## Test Your Understanding

Figure \@ref(fig:moderator-overview) shows how much respondents were exposed to an anti-smoking campaign (horizontal axis) and their attitudes towards smoking, ranging from negative (0) to positive (5, vertical axis). A third variable measures the extend to which respondents have daily contact with people who smoke. Does the contact variable moderate the effect of exposure on attitude?

```{r moderator-overview, fig.cap="How does moderation work in a regression model?", echo=FALSE, screenshot.opts = list(delay = 5), dev="png", out.width="440px"}
# Use app continuous-moderator.
knitr::include_app("http://82.196.4.233:3838/apps/continuous-moderator/", height="550px")
```

1. What does the red line in Figure \@ref(fig:moderator-overview) mean?
```{r eval=FALSE, echo=FALSE}
* The (red) regression line represents the (estimated) predictive effect of
exposure on attitude for a particular value of contact (with smokers).
* Contact is a moderator of the effect of exposure on attitude. In the initial
plot after loading the app, the value of contact is zero, so the regression
line expresses the predictive effect of exposure on attitude for respondents
who have no contact with smokers.
```

2. What happens if you change the position on the slider? Explain you answer.
```{r eval=FALSE, echo=FALSE}
* A change of the slider changes the moderator value, so the regression line
is re-estimated for respondents with another number of contacts with smokers.
As a result, the regression line is redrawn (the previous regression line is
shown in grey).
* Because the regression line represents respondents with a different
moderator score, the regression line is based on other observations (dots in
the plot). The observations with scores closest to the selected moderator
value are coloured blue. Changing the moderator value changes the relevant
observations.
* If the moderator value increases, the regression line's decrease is less
steep and at some point changes into an increase from left to right.
```

3. Why does _contact_ (with smokers) appear in between brackets together with the regression coefficient for exposure in the regression equation?
```{r eval=FALSE, echo=FALSE}
* Due to the interaction effect between exposure and contact in the model, the
predictive effect of exposure depends on the respondent's contact score. For
this reason, the respondent's contact score and its interaction regression
coefficient are included in the (conditional) predictive effect of exposure.
Thus, contact adds to (or subtracts from) the predictive effect of exposure.
* The slope of the regression line becomes less negative or more positive for
higher moderator (contact) values because the interaction effect is positive
(0.04). Every addition unit on the moderator adds 0.04 to the regression slope
of the conditional effect of exposure.
```

4. Which of the regression coefficients represent(s) a partial effect and which a conditional effect? Explain your answer.
```{r eval=FALSE, echo=FALSE}
* In this regression equation, all regression coefficients represent a partial
and a conditional effect.
* The effects are partial because the multiple regression model controls each
effect for all other effects.
* The effects are conditional because the effect of exposure on attitude
represents the effect for one value of the moderator variable contact.
* But moderation is symmetrical in the sense that we can also see exposure as
moderator of the effect of contact (join the interaction effect between
brackets with the contact effect), so the contact effect (b2) is the
predictive effect of contact for respondents scoring zero on the exposure
predictor.
```

5. What is the null hypothesis of a significance test on the interaction effect ($b_3$)? 
```{r eval=FALSE, echo=FALSE}
* The null hypothesis of an interaction effect in a (multiple) regression
model is that there is no interaction effect between these predictors at all
in the population.
* In other words, the null hypothesis states that the effect of a predictor is
the same at all levels of the other predictor(s) (included in the interaction
effect) in the population.
```

## Take-Home Points  

* In a regression model, moderation means that there are different slopes (of the predictor) for different groups or contexts (moderator).

* Interaction variables represent moderation in a regression model. 

* An interaction variable is the product of the predictor and moderator. If the moderator is categorical, it is represented by one or more dummy variables. There is an interaction variable for each of the moderator's dummy variables. 

* Statistical inference for an interaction variable is exactly the same as for "ordinary" regression predictors.

* The effect of the predictor in a model with an interaction variable does _not_ represent a main or average effect. It is a conditional effect: The effect for cases that score zero on the moderator. The same applies to the effect of the moderator, which is the conditional effect for cases scoring zero on the predictor.

* Mean center a numeric moderator and a numeric predictor that are involved in an interaction effect. Observations with a mean score on the moderator are a substantively interesting reference group.

* To interpret moderation, describe the effects (slopes, unstandardized regression coefficients) and preferably visualize the regression lines for different groups. For a numerical variable, select some interesting levels of the moderator, such as the mean and one standard deviation below or above the mean.

* Interpret regression lines for groups or moderator levels only if the predictor scores are nicely distributed for this group or level (common support). 

* Don't use the standardized regression coefficients (Beta) for interaction variables in SPSS.